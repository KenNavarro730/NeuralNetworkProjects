{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EnglishtoGerman.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YC4Mq1om1MUs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EnglishGermanData = pd.read_csv(\"deu.txt\", sep = '\\t')"
      ],
      "metadata": {
        "id": "DNJdhACe2AwS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EnglishGermanData['Go.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCenUQwO2JBh",
        "outputId": "80bcdedd-7207-42e5-93ae-12bf384738db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                       Hi.\n",
              "1                                                       Hi.\n",
              "2                                                      Run!\n",
              "3                                                      Run.\n",
              "4                                                      Wow!\n",
              "                                ...                        \n",
              "249224    If someone who doesn't know your background sa...\n",
              "249225    If someone who doesn't know your background sa...\n",
              "249226    It may be impossible to get a completely error...\n",
              "249227    I know that adding sentences only in your nati...\n",
              "249228    Doubtless there exists in this world precisely...\n",
              "Name: Go., Length: 249229, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EnglishGermanData['Geh.']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SRX6hup2KJx",
        "outputId": "db63f2e9-7445-4d7f-be79-d08db8f5f1d2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                    Hallo!\n",
              "1                                                Grüß Gott!\n",
              "2                                                     Lauf!\n",
              "3                                                     Lauf!\n",
              "4                                               Potzdonner!\n",
              "                                ...                        \n",
              "249224    Wenn jemand Fremdes dir sagt, dass du dich wie...\n",
              "249225    Wenn jemand, der nicht weiß, woher man kommt, ...\n",
              "249226    Es ist wohl unmöglich, einen vollkommen fehler...\n",
              "249227    Ich weiß wohl, dass das ausschließliche Beitra...\n",
              "249228    Ohne Zweifel findet sich auf dieser Welt zu je...\n",
              "Name: Geh., Length: 249229, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "TfEnglishGerman = tf.data.Dataset.from_tensor_slices((EnglishGermanData['Go.'], EnglishGermanData['Geh.']))"
      ],
      "metadata": {
        "id": "au1QO7AX2WwD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in TfEnglishGerman.take(2):\n",
        "  print(p[0].numpy().decode(\"utf-8\"), p[1].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r86gkO8M2rrz",
        "outputId": "7cbd9564-150e-46cd-bcb1-f180535fa9be"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi. Hallo!\n",
            "Hi. Grüß Gott!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(TfEnglishGerman))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N595NkM625XD",
        "outputId": "57233189-98b0-4197-f411-55a4654c1fdd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "249229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After inspecting the text file I was alone to see that we have around 250,000 instances in our data. It would not hurt to only use 1/5 of this, so lets just grab that many."
      ],
      "metadata": {
        "id": "ival4WZn3kwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SampledTFEnglishGerman = TfEnglishGerman.take(50000)\n",
        "print(len(SampledTFEnglishGerman))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or2eMDJ93pmF",
        "outputId": "c4879fb3-2871-4e16-e7e6-c5c93cd271bb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in SampledTFEnglishGerman.take(20):\n",
        "  print(j[0].numpy().decode(\"utf-8\"), j[1].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zlw30SK4DBm",
        "outputId": "04b38bc3-380f-473b-faf8-6c8cb0d5e01f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi. Hallo!\n",
            "Hi. Grüß Gott!\n",
            "Run! Lauf!\n",
            "Run. Lauf!\n",
            "Wow! Potzdonner!\n",
            "Wow! Donnerwetter!\n",
            "Duck! Kopf runter!\n",
            "Fire! Feuer!\n",
            "Help! Hilfe!\n",
            "Help! Zu Hülf!\n",
            "Stay. Bleib!\n",
            "Stop! Stopp!\n",
            "Stop! Anhalten!\n",
            "Wait! Warte!\n",
            "Wait. Warte.\n",
            "Begin. Fang an.\n",
            "Do it. Mache es!\n",
            "Do it. Tue es.\n",
            "Go on. Mach weiter.\n",
            "Hello! Hallo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Im curious to know if this method takes the first 50000 instances or if it takes instances at random? The docs isnt very clear so lets see if first 20 instances in original dataset\n",
        "#match the first twenty instancs in this dataset. Upon looking at the dataset I notice a bulk of the text is in the last thousands of instances. So intead lets grab from the last approx 50,000 instances\n",
        "\n",
        "for j in TfEnglishGerman.take(20):\n",
        "  print(j[0].numpy().decode(\"utf-8\"), j[1].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhJv1dpA4K6W",
        "outputId": "7b0978ed-7a35-48b3-cfca-e7da171b4805"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi. Hallo!\n",
            "Hi. Grüß Gott!\n",
            "Run! Lauf!\n",
            "Run. Lauf!\n",
            "Wow! Potzdonner!\n",
            "Wow! Donnerwetter!\n",
            "Duck! Kopf runter!\n",
            "Fire! Feuer!\n",
            "Help! Hilfe!\n",
            "Help! Zu Hülf!\n",
            "Stay. Bleib!\n",
            "Stop! Stopp!\n",
            "Stop! Anhalten!\n",
            "Wait! Warte!\n",
            "Wait. Warte.\n",
            "Begin. Fang an.\n",
            "Do it. Mache es!\n",
            "Do it. Tue es.\n",
            "Go on. Mach weiter.\n",
            "Hello! Hallo!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TrainingENDEDATA = TfEnglishGerman.skip(200000) #Grabs last 49,229 instances"
      ],
      "metadata": {
        "id": "a29crUr241gI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(TrainingENDEDATA))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWMNqoqf5jJp",
        "outputId": "432a396e-3596-407e-e81d-f45272e5691b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49229\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for z in TrainingENDEDATA.take(10):\n",
        "  print(z[0].numpy().decode(\"utf-8\"), z[1].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlVaqk7B5l7o",
        "outputId": "46c61c46-b878-4b0d-cc16-1796cd348bf7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom said that somebody here needed help. Tom sagte, dass hier jemand Hilfe brauche.\n",
            "Tom said there's nothing else we can do. Tom sagte, wir können nichts weiter machen.\n",
            "Tom said there's nothing else we can do. Tom hat gesagt, dass wir sonst nichts tun können.\n",
            "Tom said you were meeting him for lunch. Tom sagte, du wolltest dich mit ihm zum Mittagessen treffen.\n",
            "Tom says he hasn't made up his mind yet. Tom sagt, dass er sich noch nicht entschieden hat.\n",
            "Tom says he heard Mary and John arguing. Tom sagt, er habe Maria und Johannes streiten hören.\n",
            "Tom says he heard that Mary needed help. Tom meint, er habe gehört, dass Maria Hilfe brauche.\n",
            "Tom says he hopes you'll be able to win. Tom sagt, er hoffe, du gewinnest.\n",
            "Tom says he's fluent in three languages. Tom sagt, er spreche drei Sprachen fließend.\n",
            "Tom says he's given it a lot of thought. Tom sagt, er habe viel darüber nachgedacht.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are going to be making a dynamical model, we dont really need to fix the size of the data to a certain token length, they can all vary. However lets just check to see the length of the longest sentence to see what the max length is."
      ],
      "metadata": {
        "id": "X7Nair4352Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "englishmax = 0\n",
        "import numpy as np \n",
        "for obj in TrainingENDEDATA.take(len(TrainingENDEDATA)):\n",
        "  if len(obj[0].numpy().decode(\"utf-8\").split())>englishmax:\n",
        "    englishmax = len(obj[0].numpy().decode(\"utf-8\").split())\n",
        "germanmax = 0\n",
        "for obj1 in TrainingENDEDATA.take(len(TrainingENDEDATA)):\n",
        "  if len(obj[1].numpy().decode(\"utf-8\").split())> germanmax:\n",
        "    germanmax = len(obj[1].numpy().decode(\"utf-8\").split())"
      ],
      "metadata": {
        "id": "KjsGOyRs5wHK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(englishmax, germanmax) #Longest sequence of words in english data is 101 words, and in the german its 76 words. so tbh I want to tokenize these and pad to length 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1tSeshz6r8F",
        "outputId": "0495dc20-38cb-42f5-85d1-58710170948d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101 76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleanedenglish = []\n",
        "cleanedgerman = []\n",
        "for vals in TrainingENDEDATA.take(len(TrainingENDEDATA)):\n",
        "  cleanedenglish.append(vals[0].numpy().decode(\"utf-8\"))\n",
        "  cleanedgerman.append(vals[1].numpy().decode(\"utf-8\"))\n",
        "#Now after we grab the above lets remove anything thats not a a-Z letter and not a number between 0-9 \n",
        "import re \n",
        "cleanedenglish = [re.sub(r'[^a-zA-Z0-9]', ' ', entext) for entext in cleanedenglish]\n",
        "cleanedgerman = [re.sub(r'[^a-zA-Z0-9]', ' ', detext) for detext in cleanedgerman]\n",
        "#Now that this is cleaned lets put it into a dataset and prepare the tokenizer.\n",
        "CleanedENDEDATA = tf.data.Dataset.from_tensor_slices((cleanedenglish, cleanedgerman)) "
      ],
      "metadata": {
        "id": "WLogFmxK6uQK"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting the TextVectorization Process"
      ],
      "metadata": {
        "id": "mcnLFLW5ASPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "npenglish = np.array(cleanedenglish)\n",
        "npgerman = np.array(cleanedgerman)\n",
        "unique_en_words = len(set(npenglish.flatten()))\n",
        "unique_german_words = len(set(npgerman.flatten()))\n",
        "print(unique_en_words, unique_german_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8UVThqw87Qb",
        "outputId": "ee878ae4-f6b6-438a-b300-44ef49f7f251"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42117 47254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "endummydata = tf.data.Dataset.from_tensor_slices(cleanedenglish)\n",
        "dedummydata = tf.data.Dataset.from_tensor_slices(cleanedgerman)\n",
        "enTextVectorizer = tf.keras.layers.TextVectorization(max_tokens = unique_en_words, standardize = 'lower_and_strip_punctuation', output_mode='int', output_sequence_length=128,\n",
        "                                                     pad_to_max_tokens=True)\n",
        "deTextVectorizer = tf.keras.layers.TextVectorization(max_tokens = unique_german_words, standardize='lower_and_strip_punctuation', output_mode='int', output_sequence_length=128,\n",
        "                                                     pad_to_max_tokens=True)\n",
        "enTextVectorizer.adapt(endummydata)\n",
        "deTextVectorizer.adapt(dedummydata)"
      ],
      "metadata": {
        "id": "xo4MD0H58tzd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TokenizeDataset(english, german):\n",
        "  return enTextVectorizer(english), deTextVectorizer(german)\n",
        "CleanedENDEDATA = CleanedENDEDATA.cache().shuffle(len(CleanedENDEDATA)).batch(64).map(TokenizeDataset, num_parallel_calls = tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "UzTjLKWF_8mO"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokenized in CleanedENDEDATA.take(1):\n",
        "  print(tokenized[0], tokenized[1]) #Fair enough! So lets now take a look to make sure the length is still proper, it should be originallength/64"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqiZXyKDCYSG",
        "outputId": "dda259b7-ad8e-4a83-f27a-610edaec34e8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[   5 4536  944 ...    0    0    0]\n",
            " [  28   78  154 ...    0    0    0]\n",
            " [  22  702   13 ...    0    0    0]\n",
            " ...\n",
            " [   2  325  103 ...    0    0    0]\n",
            " [   4  445   11 ...    0    0    0]\n",
            " [   5  495    8 ...    0    0    0]], shape=(64, 128), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(CleanedENDEDATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flsFSkw4Cdhe",
        "outputId": "05ae7180-c30c-46a2-a2f3-0d386d28885f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "770"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(49229/64) #Lets remove the last batch! since they rounded up."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l84xfV0EC2se",
        "outputId": "d04ba5c1-aefd-4a68-e65a-8fe0b948fd19"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "769.203125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CleanedENDEDATA = CleanedENDEDATA.take(769)"
      ],
      "metadata": {
        "id": "RRMN9nCeC7z-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(CleanedENDEDATA)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Sye9ZmSDHte",
        "outputId": "5b442450-964b-4e67-ffab-e63519194485"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "769"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Our data seems to be ready, its in tuple form, batches, parallelized, and prepped.\n",
        "#Time to make our model.\n",
        "def point_wise_feed_forward_network(dff, d_model):\n",
        "  return tf.keras.Sequential([\n",
        "          tf.keras.layers.Dense(dff, activation = 'relu'),\n",
        "          tf.keras.layers.Dense(d_model) \n",
        "  ])"
      ],
      "metadata": {
        "id": "nzgtpUaNDJMe"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, v, k, mask):\n",
        "  scores = tf.linalg.matmul(q,k, transpose_b=True)\n",
        "  scaledscores = scores/tf.math.sqrt(tf.cast((tf.shape(v)[-1]), tf.float32))\n",
        "  if mask is not None:\n",
        "    scaledscores += (mask * -1e9)\n",
        "  softmaxscores = tf.nn.softmax(scaledscores)\n",
        "  return tf.matmul(softmaxscores, v), softmaxscores"
      ],
      "metadata": {
        "id": "YFYTo8KgGPpN"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding\n",
        "## Formula: cos for odds, sin for even pos/(10000^((2*i)/d_model))\n"
      ],
      "metadata": {
        "id": "B4Fcyzh2HVe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def innerpart(position, iofd, d_model):\n",
        "  return position/(np.power(10000, (2*iofd)/d_model))\n",
        "def positional_encoding(maximum_position, d_model):\n",
        "  untrigedarr = innerpart(np.arange(maximum_position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
        "  untrigedarr[:, 0::2] = np.sin(untrigedarr[:, 0::2])\n",
        "  untrigedarr[:, 1::2] = np.cos(untrigedarr[:, 1::2])\n",
        "  return tf.cast(untrigedarr[np.newaxis, ... ], tf.float32) #We raise the dimension for the sake of tensorflow, as the second index refers to rows and the third refers to columns.\n",
        "  #The first index refers to batch index."
      ],
      "metadata": {
        "id": "b60wVaGfHmdA"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return look_ahead_mask\n",
        "def create_padding_mask(sequence):\n",
        "  values = tf.cast(tf.math.equal(sequence, 0), tf.float32)\n",
        "  return values[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "Gl2UsMjbL1Va"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads, d_model):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model \n",
        "    assert self.d_model % self.num_heads == 0 #Throws an error if the dimensionality of our vector embeddings are not divisible by the number of heads we select. \n",
        "    self.depth = self.d_model//self.num_heads \n",
        "    self.wQ = tf.keras.layers.Dense(d_model)\n",
        "    self.wK = tf.keras.layers.Dense(d_model)\n",
        "    self.wV = tf.keras.layers.Dense(d_model)\n",
        "    self.finaloutput = tf.keras.layers.Dense(d_model)\n",
        "  def reshape(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    x = tf.transpose(x, perm = [0,2,1,3]) #Goes from batch -> seq_len->num_heads->depth to batch->num_heads -> seq_len -> depth\n",
        "    return x\n",
        "  def call(self, query, key, value, mask):\n",
        "    batch_size = tf.shape(query)[0]\n",
        "    q = self.wQ(query)\n",
        "    k = self.wK(key)\n",
        "    v = self.wV(value)\n",
        "    q = self.reshape(q, batch_size)\n",
        "    v = self.reshape(v, batch_size)\n",
        "    k = self.reshape(k, batch_size) \n",
        "    attentionout, attentionweights = scaled_dot_product_attention(q, v, k, mask)\n",
        "    attentionout = tf.transpose(attentionout, perm=[0,2,1,3])\n",
        "    attentionout = tf.reshape(attentionout, (batch_size, -1, self.d_model)) #This concatenates the attention pieces again.\n",
        "    linearout = self.finaloutput(attentionout)\n",
        "    return linearout, attentionweights"
      ],
      "metadata": {
        "id": "at_EokgsIicj"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads, d_model, dff, rate = 0.05):\n",
        "    super(EncoderLayer, self).__init__() \n",
        "    self.d_model = d_model \n",
        "    self.num_heads = num_heads\n",
        "    self.ffn = point_wise_feed_forward_network(dff, d_model)\n",
        "    self.mha= MultiHeadAttention(self.num_heads, self.d_model)\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization()\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "  def call(self, input,Training,  padding_mask):\n",
        "    x, _ = self.mha(input, input, input, padding_mask)\n",
        "    x = self.dropout1(x, training = Training)\n",
        "    x = self.layernorm1(x + input)\n",
        "    ffn = self.ffn(x)\n",
        "    ffn = self.dropout2(ffn, training = Training)\n",
        "    ffn = self.layernorm2(ffn + x)\n",
        "    return ffn"
      ],
      "metadata": {
        "id": "1vmmT5PKNhOr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoder_layer = EncoderLayer(8,512, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iKEdyYWSBqm",
        "outputId": "9898b7a3-186b-4cd3-91e7-b1621a06dc42"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate = 0.05):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads \n",
        "    self.ffn = point_wise_feed_forward_network(dff, d_model)\n",
        "    self.mha1 = MultiHeadAttention(num_heads, d_model)\n",
        "    self.mha2 = MultiHeadAttention(num_heads, d_model)\n",
        "    self.ln1 = tf.keras.layers.LayerNormalization()\n",
        "    self.ln2 = tf.keras.layers.LayerNormalization()\n",
        "    self.ln3 = tf.keras.layers.LayerNormalization()\n",
        "    self.do1 = tf.keras.layers.Dropout(rate)\n",
        "    self.do2 = tf.keras.layers.Dropout(rate)\n",
        "    self.do3 = tf.keras.layers.Dropout(rate)\n",
        "  def call(self, input, enc_output,training, padding_mask, look_ahead_mask):\n",
        "    x, attnw1 = self.mha1(input, input, input, look_ahead_mask)\n",
        "    x = self.do1(x, training = training)\n",
        "    x = self.ln1(x + input)\n",
        "    x2,attnw2 = self.mha2(x, enc_output, enc_output, padding_mask)\n",
        "    x2 = self.do2(x2, training = training)\n",
        "    x2 = self.ln2(x2 + x)\n",
        "    ffn1 = self.ffn(x2)\n",
        "    ffn1 = self.do3(ffn1, training = training)\n",
        "    ffn1 = self.ln3(ffn1 + x2)\n",
        "    return ffn1, attnw1, attnw2"
      ],
      "metadata": {
        "id": "Kxx-GRaOPi3i"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay5dRYzcT56z",
        "outputId": "9750ded1-f61e-4744-a4b7-a9bcec715646"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, num_layers, dff, input_vocab_size, maximum_position_encoding):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.num_layers = num_layers \n",
        "    self.d_model = d_model \n",
        "    self.enc_layers = [EncoderLayer(num_heads, d_model, dff) for _ in range(num_layers)]\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim = input_vocab_size, output_dim = d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "  def call(self, input, training, mask):\n",
        "    seq_len = tf.shape(input)[1] \n",
        "    embeddedin = self.embedding(input)\n",
        "    embeddedin *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #Check if this is required too\n",
        "    embeddedin += self.pos_encoding[:, :seq_len, :] #Since we have all sequences to length 128 we can check after if this argument is really necessary to get a valid training in.\n",
        "    embeddedin = self.dropout(embeddedin)\n",
        "    output = embeddedin \n",
        "    for z in range(self.num_layers):\n",
        "      output = self.enc_layers[z](output, training, padding_mask=mask)\n",
        "    return output "
      ],
      "metadata": {
        "id": "9BbJn_NKT8H7"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_encoder = Encoder(d_model=512, num_heads=8, num_layers=2,\n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nyrq7yHsXLp7",
        "outputId": "862d1e9f-6a13-490e-9282-084a00d0e266"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 62, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, num_layers, dff, target_vocab_size, maximum_position_encoding):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.d_model = d_model \n",
        "    self.num_heads = num_heads\n",
        "    self.num_layers = num_layers\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(self.num_layers)]\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim = target_vocab_size, output_dim = d_model)\n",
        "  def call(self, input, enc_output, training, padding_mask, look_ahead_mask):\n",
        "    seq_len = tf.shape(input)[1]\n",
        "    x = self.embedding(input)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model ,tf.float32))\n",
        "    attentionlist = []\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    for z in range(self.num_layers):\n",
        "      x, attnb1, attnb2 = self.dec_layers[z](x, enc_output, training, padding_mask, look_ahead_mask)\n",
        "      attentionlist.append(attnb1)\n",
        "      attentionlist.append(attnb2)\n",
        "    return x, attentionlist"
      ],
      "metadata": {
        "id": "xZ865tw2XShK"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_decoder = Decoder(d_model=512, num_heads=8,num_layers=2,\n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c-1V4a7ZN1y",
        "outputId": "dd48df4b-0c6a-4da1-d93a-ea5f87b6a24b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 26, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate = 0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(d_model, num_heads, num_layers, dff, input_vocab_size, pe_input)\n",
        "    self.decoder = Decoder(d_model, num_heads, num_layers, dff, target_vocab_size, pe_target)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size) #Since we are predicing over all german words\n",
        "  def call(self, inputs, training):\n",
        "    x, y = inputs  #Since we have a tuple as our dataset\n",
        "    enc_pad_mask, dec_look_ahead_mask, dec_padding_mask = self.create_masks(x, y)\n",
        "    enc_out = self.encoder(x, training, enc_pad_mask)\n",
        "    dec_out, attn = self.decoder(y, enc_out, training, dec_padding_mask, dec_look_ahead_mask)\n",
        "    finaloutput = self.final_layer(dec_out)\n",
        "    return finaloutput, attn\n",
        "  def create_masks(self, inp, tar):\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    dec_padding_mask = create_padding_mask(inp) #Second attention block in decoder\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, look_ahead_mask, dec_padding_mask "
      ],
      "metadata": {
        "id": "exJ40iWzZRy6"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer([temp_input, temp_target], training=False)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaHlaXuvbrsf",
        "outputId": "bea49f16-9662-4973-d38c-200d88dd8574"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "pPhbzFy9bs8p"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(LearningRateSchedule, self).__init__()\n",
        "    self.d_model = d_model \n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    self.warmup_steps =warmup_steps\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d_model)*tf.math.minimum(arg1,arg2)"
      ],
      "metadata": {
        "id": "EFW8mWcfcCP5"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = LearningRateSchedule(d_model), epsilon = 1e-9)\n"
      ],
      "metadata": {
        "id": "xIcYy8XgclVp"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_learning_rate_schedule = LearningRateSchedule(d_model)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "cCrTQWRKcuvZ",
        "outputId": "06fdef1e-561d-4dc3-9b0c-f97edb5e0c2b"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
      ],
      "metadata": {
        "id": "NnmwGVhJcyL5"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(real, pred):\n",
        "  diff = loss(real, pred)\n",
        "  return tf.reduce_sum(diff)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')"
      ],
      "metadata": {
        "id": "ni6LDlojc8yo"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer1 = Transformer(num_layers=num_layers, d_model = d_model, num_heads = num_heads, dff= dff, input_vocab_size = unique_en_words, target_vocab_size = unique_german_words,\n",
        "                           pe_input= 1000, pe_target=1000)"
      ],
      "metadata": {
        "id": "d-RHkMF4dNvI"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer1,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "metadata": {
        "id": "qYqludQVeJZp"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "T6nrdLBweL74"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer1([inp, tar_inp],\n",
        "                                 training = True)\n",
        "    loss = loss_func(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer1.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer1.trainable_variables))\n",
        "\n",
        "  train_loss(loss)"
      ],
      "metadata": {
        "id": "7Oq6NmbTePAX"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PJcvy0Nesz3",
        "outputId": "861baba7-e8e1-4db7-9a0d-bff6f137326a"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.8,>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.23.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.43.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (12.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.8,>=2.7.0->tensorflow_text) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (57.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.8,>=2.7.0->tensorflow_text) (3.1.1)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "tqyRFeLje2Oq"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "\n",
        "  # inp -> portuguese, tar -> english\n",
        "  for (batch, (inp, tar)) in enumerate(CleanedENDEDATA):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmxVgIW4edhf",
        "outputId": "0e7185af-dde2-469c-af21-beb7f64f81ea"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 86856.4375\n",
            "Epoch 1 Batch 50 Loss 85372.7031\n",
            "Epoch 1 Batch 100 Loss 83038.8594\n",
            "Epoch 1 Batch 150 Loss 80909.9922\n",
            "Epoch 1 Batch 200 Loss 78459.7344\n",
            "Epoch 1 Batch 250 Loss 75484.2344\n",
            "Epoch 1 Batch 300 Loss 71918.3828\n",
            "Epoch 1 Batch 350 Loss 67723.4609\n",
            "Epoch 1 Batch 400 Loss 62881.9219\n",
            "Epoch 1 Batch 450 Loss 57669.7031\n",
            "Epoch 1 Batch 500 Loss 52825.3789\n",
            "Epoch 1 Batch 550 Loss 48706.3516\n",
            "Epoch 1 Batch 600 Loss 45187.1680\n",
            "Epoch 1 Batch 650 Loss 42146.6562\n",
            "Epoch 1 Batch 700 Loss 39510.9141\n",
            "Epoch 1 Batch 750 Loss 37207.5430\n",
            "Epoch 1 Loss 36448.9922\n",
            "Time taken for 1 epoch: 248.17 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4897.9072\n",
            "Epoch 2 Batch 50 Loss 4720.4189\n",
            "Epoch 2 Batch 100 Loss 4676.4473\n",
            "Epoch 2 Batch 150 Loss 4632.2920\n",
            "Epoch 2 Batch 200 Loss 4589.7632\n",
            "Epoch 2 Batch 250 Loss 4553.4028\n",
            "Epoch 2 Batch 300 Loss 4529.7446\n",
            "Epoch 2 Batch 350 Loss 4502.7759\n",
            "Epoch 2 Batch 400 Loss 4472.0791\n",
            "Epoch 2 Batch 450 Loss 4446.1099\n",
            "Epoch 2 Batch 500 Loss 4423.1313\n",
            "Epoch 2 Batch 550 Loss 4399.2222\n",
            "Epoch 2 Batch 600 Loss 4370.7036\n",
            "Epoch 2 Batch 650 Loss 4342.4648\n",
            "Epoch 2 Batch 700 Loss 4314.9272\n",
            "Epoch 2 Batch 750 Loss 4288.6533\n",
            "Epoch 2 Loss 4279.0840\n",
            "Time taken for 1 epoch: 246.40 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4063.4744\n",
            "Epoch 3 Batch 50 Loss 3726.3691\n",
            "Epoch 3 Batch 100 Loss 3721.9216\n",
            "Epoch 3 Batch 150 Loss 3709.4053\n",
            "Epoch 3 Batch 200 Loss 3679.1797\n",
            "Epoch 3 Batch 250 Loss 3655.6436\n",
            "Epoch 3 Batch 300 Loss 3640.6357\n",
            "Epoch 3 Batch 350 Loss 3617.9976\n",
            "Epoch 3 Batch 400 Loss 3596.4678\n",
            "Epoch 3 Batch 450 Loss 3575.1697\n",
            "Epoch 3 Batch 500 Loss 3552.2585\n",
            "Epoch 3 Batch 550 Loss 3526.8210\n",
            "Epoch 3 Batch 600 Loss 3505.9529\n",
            "Epoch 3 Batch 650 Loss 3484.4861\n",
            "Epoch 3 Batch 700 Loss 3466.2200\n",
            "Epoch 3 Batch 750 Loss 3446.0566\n",
            "Epoch 3 Loss 3438.7610\n",
            "Time taken for 1 epoch: 245.42 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3037.0564\n",
            "Epoch 4 Batch 50 Loss 2978.9180\n",
            "Epoch 4 Batch 100 Loss 2985.2512\n",
            "Epoch 4 Batch 150 Loss 2966.6436\n",
            "Epoch 4 Batch 200 Loss 2951.8516\n",
            "Epoch 4 Batch 250 Loss 2943.6475\n",
            "Epoch 4 Batch 300 Loss 2930.9685\n",
            "Epoch 4 Batch 350 Loss 2920.2312\n",
            "Epoch 4 Batch 400 Loss 2908.7986\n",
            "Epoch 4 Batch 450 Loss 2895.8569\n",
            "Epoch 4 Batch 500 Loss 2884.7485\n",
            "Epoch 4 Batch 550 Loss 2871.5654\n",
            "Epoch 4 Batch 600 Loss 2859.6926\n",
            "Epoch 4 Batch 650 Loss 2849.7991\n",
            "Epoch 4 Batch 700 Loss 2837.7439\n",
            "Epoch 4 Batch 750 Loss 2825.8313\n",
            "Epoch 4 Loss 2822.4197\n",
            "Time taken for 1 epoch: 250.34 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2313.0034\n",
            "Epoch 5 Batch 50 Loss 2433.3611\n",
            "Epoch 5 Batch 100 Loss 2404.0994\n",
            "Epoch 5 Batch 150 Loss 2384.3718\n",
            "Epoch 5 Batch 200 Loss 2367.6035\n",
            "Epoch 5 Batch 250 Loss 2360.8835\n",
            "Epoch 5 Batch 300 Loss 2348.9392\n",
            "Epoch 5 Batch 350 Loss 2337.3511\n",
            "Epoch 5 Batch 400 Loss 2324.6611\n",
            "Epoch 5 Batch 450 Loss 2317.2400\n",
            "Epoch 5 Batch 500 Loss 2309.2322\n",
            "Epoch 5 Batch 550 Loss 2305.7007\n",
            "Epoch 5 Batch 600 Loss 2296.5457\n",
            "Epoch 5 Batch 650 Loss 2286.6104\n",
            "Epoch 5 Batch 700 Loss 2278.3474\n",
            "Epoch 5 Batch 750 Loss 2269.2874\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 2265.4382\n",
            "Time taken for 1 epoch: 251.43 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1900.7627\n",
            "Epoch 6 Batch 50 Loss 1794.7266\n",
            "Epoch 6 Batch 100 Loss 1837.6312\n",
            "Epoch 6 Batch 150 Loss 1849.2206\n",
            "Epoch 6 Batch 200 Loss 1852.0907\n",
            "Epoch 6 Batch 250 Loss 1853.1998\n",
            "Epoch 6 Batch 300 Loss 1854.4579\n",
            "Epoch 6 Batch 350 Loss 1849.1558\n",
            "Epoch 6 Batch 400 Loss 1846.5046\n",
            "Epoch 6 Batch 450 Loss 1842.5409\n",
            "Epoch 6 Batch 500 Loss 1840.8716\n",
            "Epoch 6 Batch 550 Loss 1838.0393\n",
            "Epoch 6 Batch 600 Loss 1835.7051\n",
            "Epoch 6 Batch 650 Loss 1833.2581\n",
            "Epoch 6 Batch 700 Loss 1830.0773\n",
            "Epoch 6 Batch 750 Loss 1824.4250\n",
            "Epoch 6 Loss 1822.7646\n",
            "Time taken for 1 epoch: 250.23 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1384.3022\n",
            "Epoch 7 Batch 50 Loss 1459.2909\n",
            "Epoch 7 Batch 100 Loss 1464.8910\n",
            "Epoch 7 Batch 150 Loss 1458.3623\n",
            "Epoch 7 Batch 200 Loss 1455.3521\n",
            "Epoch 7 Batch 250 Loss 1455.3730\n",
            "Epoch 7 Batch 300 Loss 1457.8671\n",
            "Epoch 7 Batch 350 Loss 1456.4175\n",
            "Epoch 7 Batch 400 Loss 1458.8795\n",
            "Epoch 7 Batch 450 Loss 1458.7726\n",
            "Epoch 7 Batch 500 Loss 1457.7391\n",
            "Epoch 7 Batch 550 Loss 1456.2375\n",
            "Epoch 7 Batch 600 Loss 1452.7563\n",
            "Epoch 7 Batch 650 Loss 1452.5193\n",
            "Epoch 7 Batch 700 Loss 1451.4012\n",
            "Epoch 7 Batch 750 Loss 1453.7029\n",
            "Epoch 7 Loss 1453.6989\n",
            "Time taken for 1 epoch: 250.40 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1162.3473\n",
            "Epoch 8 Batch 50 Loss 1140.8755\n",
            "Epoch 8 Batch 100 Loss 1150.0431\n",
            "Epoch 8 Batch 150 Loss 1149.5632\n",
            "Epoch 8 Batch 200 Loss 1155.9353\n",
            "Epoch 8 Batch 250 Loss 1159.6256\n",
            "Epoch 8 Batch 300 Loss 1162.7948\n",
            "Epoch 8 Batch 350 Loss 1166.5448\n",
            "Epoch 8 Batch 400 Loss 1167.1522\n",
            "Epoch 8 Batch 450 Loss 1172.0981\n",
            "Epoch 8 Batch 500 Loss 1175.8335\n",
            "Epoch 8 Batch 550 Loss 1177.6002\n",
            "Epoch 8 Batch 600 Loss 1178.8333\n",
            "Epoch 8 Batch 650 Loss 1178.1366\n",
            "Epoch 8 Batch 700 Loss 1180.6547\n",
            "Epoch 8 Batch 750 Loss 1182.5188\n",
            "Epoch 8 Loss 1182.6801\n",
            "Time taken for 1 epoch: 250.51 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1022.3834\n",
            "Epoch 9 Batch 50 Loss 899.4925\n",
            "Epoch 9 Batch 100 Loss 910.2698\n",
            "Epoch 9 Batch 150 Loss 916.5264\n",
            "Epoch 9 Batch 200 Loss 927.1384\n",
            "Epoch 9 Batch 250 Loss 937.0207\n",
            "Epoch 9 Batch 300 Loss 940.9727\n",
            "Epoch 9 Batch 350 Loss 946.0584\n",
            "Epoch 9 Batch 400 Loss 952.1208\n",
            "Epoch 9 Batch 450 Loss 955.5564\n",
            "Epoch 9 Batch 500 Loss 960.4010\n",
            "Epoch 9 Batch 550 Loss 963.6835\n",
            "Epoch 9 Batch 600 Loss 966.5683\n",
            "Epoch 9 Batch 650 Loss 969.8514\n",
            "Epoch 9 Batch 700 Loss 973.6355\n",
            "Epoch 9 Batch 750 Loss 975.6998\n",
            "Epoch 9 Loss 976.5056\n",
            "Time taken for 1 epoch: 249.83 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 714.3347\n",
            "Epoch 10 Batch 50 Loss 756.7495\n",
            "Epoch 10 Batch 100 Loss 763.1631\n",
            "Epoch 10 Batch 150 Loss 765.8019\n",
            "Epoch 10 Batch 200 Loss 773.3307\n",
            "Epoch 10 Batch 250 Loss 778.0918\n",
            "Epoch 10 Batch 300 Loss 780.4142\n",
            "Epoch 10 Batch 350 Loss 787.2970\n",
            "Epoch 10 Batch 400 Loss 792.8311\n",
            "Epoch 10 Batch 450 Loss 799.8004\n",
            "Epoch 10 Batch 500 Loss 804.2234\n",
            "Epoch 10 Batch 550 Loss 810.3856\n",
            "Epoch 10 Batch 600 Loss 813.7123\n",
            "Epoch 10 Batch 650 Loss 816.8143\n",
            "Epoch 10 Batch 700 Loss 818.8233\n",
            "Epoch 10 Batch 750 Loss 821.4221\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 821.9940\n",
            "Time taken for 1 epoch: 249.81 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 739.8652\n",
            "Epoch 11 Batch 50 Loss 619.8026\n",
            "Epoch 11 Batch 100 Loss 637.0091\n",
            "Epoch 11 Batch 150 Loss 647.0059\n",
            "Epoch 11 Batch 200 Loss 651.8459\n",
            "Epoch 11 Batch 250 Loss 656.0379\n",
            "Epoch 11 Batch 300 Loss 660.5376\n",
            "Epoch 11 Batch 350 Loss 665.3564\n",
            "Epoch 11 Batch 400 Loss 673.2066\n",
            "Epoch 11 Batch 450 Loss 679.8961\n",
            "Epoch 11 Batch 500 Loss 683.4395\n",
            "Epoch 11 Batch 550 Loss 688.6534\n",
            "Epoch 11 Batch 600 Loss 693.3434\n",
            "Epoch 11 Batch 650 Loss 697.1059\n",
            "Epoch 11 Batch 700 Loss 700.8340\n",
            "Epoch 11 Batch 750 Loss 705.4419\n",
            "Epoch 11 Loss 706.6934\n",
            "Time taken for 1 epoch: 249.40 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 551.6918\n",
            "Epoch 12 Batch 50 Loss 551.8674\n",
            "Epoch 12 Batch 100 Loss 555.1107\n",
            "Epoch 12 Batch 150 Loss 556.1795\n",
            "Epoch 12 Batch 200 Loss 563.4838\n",
            "Epoch 12 Batch 250 Loss 571.9514\n",
            "Epoch 12 Batch 300 Loss 576.2001\n",
            "Epoch 12 Batch 350 Loss 580.6404\n",
            "Epoch 12 Batch 400 Loss 586.2311\n",
            "Epoch 12 Batch 450 Loss 591.6409\n",
            "Epoch 12 Batch 500 Loss 594.0253\n",
            "Epoch 12 Batch 550 Loss 598.9633\n",
            "Epoch 12 Batch 600 Loss 603.5439\n",
            "Epoch 12 Batch 650 Loss 607.4102\n",
            "Epoch 12 Batch 700 Loss 611.8156\n",
            "Epoch 12 Batch 750 Loss 615.8672\n",
            "Epoch 12 Loss 616.7551\n",
            "Time taken for 1 epoch: 248.87 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 600.3553\n",
            "Epoch 13 Batch 50 Loss 487.7319\n",
            "Epoch 13 Batch 100 Loss 483.8745\n",
            "Epoch 13 Batch 150 Loss 487.2567\n",
            "Epoch 13 Batch 200 Loss 494.1692\n",
            "Epoch 13 Batch 250 Loss 499.9945\n",
            "Epoch 13 Batch 300 Loss 503.7367\n",
            "Epoch 13 Batch 350 Loss 507.6952\n",
            "Epoch 13 Batch 400 Loss 513.2284\n",
            "Epoch 13 Batch 450 Loss 518.3835\n",
            "Epoch 13 Batch 500 Loss 523.0106\n",
            "Epoch 13 Batch 550 Loss 527.2905\n",
            "Epoch 13 Batch 600 Loss 532.4664\n",
            "Epoch 13 Batch 650 Loss 537.8051\n",
            "Epoch 13 Batch 700 Loss 541.7821\n",
            "Epoch 13 Batch 750 Loss 545.9300\n",
            "Epoch 13 Loss 547.0906\n",
            "Time taken for 1 epoch: 249.00 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 443.4167\n",
            "Epoch 14 Batch 50 Loss 436.3631\n",
            "Epoch 14 Batch 100 Loss 433.8979\n",
            "Epoch 14 Batch 150 Loss 438.4230\n",
            "Epoch 14 Batch 200 Loss 443.0864\n",
            "Epoch 14 Batch 250 Loss 448.6202\n",
            "Epoch 14 Batch 300 Loss 453.2390\n",
            "Epoch 14 Batch 350 Loss 458.6308\n",
            "Epoch 14 Batch 400 Loss 462.3601\n",
            "Epoch 14 Batch 450 Loss 466.4711\n",
            "Epoch 14 Batch 500 Loss 471.5001\n",
            "Epoch 14 Batch 550 Loss 475.5529\n",
            "Epoch 14 Batch 600 Loss 478.8385\n",
            "Epoch 14 Batch 650 Loss 483.1205\n",
            "Epoch 14 Batch 700 Loss 486.7397\n",
            "Epoch 14 Batch 750 Loss 489.6912\n",
            "Epoch 14 Loss 490.4948\n",
            "Time taken for 1 epoch: 245.68 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 425.3877\n",
            "Epoch 15 Batch 50 Loss 380.3958\n",
            "Epoch 15 Batch 100 Loss 386.5414\n",
            "Epoch 15 Batch 150 Loss 391.6137\n",
            "Epoch 15 Batch 200 Loss 398.4452\n",
            "Epoch 15 Batch 250 Loss 401.7636\n",
            "Epoch 15 Batch 300 Loss 407.2960\n",
            "Epoch 15 Batch 350 Loss 412.1372\n",
            "Epoch 15 Batch 400 Loss 416.3034\n",
            "Epoch 15 Batch 450 Loss 420.9016\n",
            "Epoch 15 Batch 500 Loss 424.5702\n",
            "Epoch 15 Batch 550 Loss 429.4131\n",
            "Epoch 15 Batch 600 Loss 431.5114\n",
            "Epoch 15 Batch 650 Loss 435.6027\n",
            "Epoch 15 Batch 700 Loss 439.0633\n",
            "Epoch 15 Batch 750 Loss 442.3060\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 443.5667\n",
            "Time taken for 1 epoch: 245.41 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 382.6871\n",
            "Epoch 16 Batch 50 Loss 348.6514\n",
            "Epoch 16 Batch 100 Loss 353.3905\n",
            "Epoch 16 Batch 150 Loss 359.9638\n",
            "Epoch 16 Batch 200 Loss 363.6780\n",
            "Epoch 16 Batch 250 Loss 367.2972\n",
            "Epoch 16 Batch 300 Loss 373.3412\n",
            "Epoch 16 Batch 350 Loss 375.8061\n",
            "Epoch 16 Batch 400 Loss 379.4053\n",
            "Epoch 16 Batch 450 Loss 383.2748\n",
            "Epoch 16 Batch 500 Loss 386.7344\n",
            "Epoch 16 Batch 550 Loss 391.4606\n",
            "Epoch 16 Batch 600 Loss 394.1057\n",
            "Epoch 16 Batch 650 Loss 396.4701\n",
            "Epoch 16 Batch 700 Loss 400.3322\n",
            "Epoch 16 Batch 750 Loss 402.9750\n",
            "Epoch 16 Loss 404.0656\n",
            "Time taken for 1 epoch: 245.45 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 303.1335\n",
            "Epoch 17 Batch 50 Loss 314.6262\n",
            "Epoch 17 Batch 100 Loss 318.9883\n",
            "Epoch 17 Batch 150 Loss 326.0627\n",
            "Epoch 17 Batch 200 Loss 328.5101\n",
            "Epoch 17 Batch 250 Loss 333.7273\n",
            "Epoch 17 Batch 300 Loss 337.6244\n",
            "Epoch 17 Batch 350 Loss 341.1366\n",
            "Epoch 17 Batch 400 Loss 343.2634\n",
            "Epoch 17 Batch 450 Loss 347.3498\n",
            "Epoch 17 Batch 500 Loss 351.9145\n",
            "Epoch 17 Batch 550 Loss 356.0086\n",
            "Epoch 17 Batch 600 Loss 360.2671\n",
            "Epoch 17 Batch 650 Loss 363.2149\n",
            "Epoch 17 Batch 700 Loss 366.5890\n",
            "Epoch 17 Batch 750 Loss 369.4130\n",
            "Epoch 17 Loss 370.0575\n",
            "Time taken for 1 epoch: 246.56 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 307.0418\n",
            "Epoch 18 Batch 50 Loss 288.6197\n",
            "Epoch 18 Batch 100 Loss 287.8866\n",
            "Epoch 18 Batch 150 Loss 292.7299\n",
            "Epoch 18 Batch 200 Loss 294.4472\n",
            "Epoch 18 Batch 250 Loss 296.9096\n",
            "Epoch 18 Batch 300 Loss 302.6958\n",
            "Epoch 18 Batch 350 Loss 305.7654\n",
            "Epoch 18 Batch 400 Loss 309.3317\n",
            "Epoch 18 Batch 450 Loss 313.9879\n",
            "Epoch 18 Batch 500 Loss 318.3589\n",
            "Epoch 18 Batch 550 Loss 321.7483\n",
            "Epoch 18 Batch 600 Loss 326.0636\n",
            "Epoch 18 Batch 650 Loss 329.8830\n",
            "Epoch 18 Batch 700 Loss 334.0009\n",
            "Epoch 18 Batch 750 Loss 337.3051\n",
            "Epoch 18 Loss 338.7666\n",
            "Time taken for 1 epoch: 247.31 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 258.2560\n",
            "Epoch 19 Batch 50 Loss 282.1319\n",
            "Epoch 19 Batch 100 Loss 277.1234\n",
            "Epoch 19 Batch 150 Loss 278.5779\n",
            "Epoch 19 Batch 200 Loss 278.0600\n",
            "Epoch 19 Batch 250 Loss 279.9322\n",
            "Epoch 19 Batch 300 Loss 284.9280\n",
            "Epoch 19 Batch 350 Loss 289.4505\n",
            "Epoch 19 Batch 400 Loss 293.4939\n",
            "Epoch 19 Batch 450 Loss 295.9656\n",
            "Epoch 19 Batch 500 Loss 298.8378\n",
            "Epoch 19 Batch 550 Loss 301.8148\n",
            "Epoch 19 Batch 600 Loss 304.5779\n",
            "Epoch 19 Batch 650 Loss 307.6465\n",
            "Epoch 19 Batch 700 Loss 311.0779\n",
            "Epoch 19 Batch 750 Loss 314.1766\n",
            "Epoch 19 Loss 315.3874\n",
            "Time taken for 1 epoch: 249.54 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 213.8359\n",
            "Epoch 20 Batch 50 Loss 252.5786\n",
            "Epoch 20 Batch 100 Loss 251.7906\n",
            "Epoch 20 Batch 150 Loss 252.9821\n",
            "Epoch 20 Batch 200 Loss 256.0180\n",
            "Epoch 20 Batch 250 Loss 259.4295\n",
            "Epoch 20 Batch 300 Loss 262.3974\n",
            "Epoch 20 Batch 350 Loss 265.8565\n",
            "Epoch 20 Batch 400 Loss 269.6733\n",
            "Epoch 20 Batch 450 Loss 272.7554\n",
            "Epoch 20 Batch 500 Loss 276.9923\n",
            "Epoch 20 Batch 550 Loss 279.9988\n",
            "Epoch 20 Batch 600 Loss 282.4910\n",
            "Epoch 20 Batch 650 Loss 284.9296\n",
            "Epoch 20 Batch 700 Loss 288.5854\n",
            "Epoch 20 Batch 750 Loss 291.5441\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 292.4767\n",
            "Time taken for 1 epoch: 247.95 secs\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p saved_model\n",
        "transformer1.save('saved_model/my_transformer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlnonBjze3mH",
        "outputId": "4fd7649e-4510-40d6-d581-2abb017be1a5"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.layers.core.dropout.Dropout object at 0x7f7e4f7f6dd0>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_11_layer_call_fn, embedding_11_layer_call_and_return_conditional_losses, dropout_109_layer_call_fn, dropout_109_layer_call_and_return_conditional_losses, embedding_12_layer_call_fn while saving (showing 5 of 555). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_transformer/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/my_transformer/assets\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4fb14d50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4e1efd90> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4e1ef310> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f444410> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f804f50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f871f50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f6a0c10> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f6cfc50> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f6beb10> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f6db2d0> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f6be050> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<__main__.MultiHeadAttention object at 0x7f7e4f77b550> has the same name 'MultiHeadAttention' as a built-in Keras object. Consider renaming <class '__main__.MultiHeadAttention'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HZKzswRKyJaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}