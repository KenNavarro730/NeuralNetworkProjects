{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyWeirdTransformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Built a random transformer based model. Just to show comfortability with tensorflow mainly. This is pretty simple stuff at this point. All you have to do now is just preprocess some data for a tast like text generation, create the encoder decoder and look ahead masks. \n",
        "Now that I think about it i think its better to actually incorporate the mask building into each step of the model. I will be fixing this up in my next rendition of the model.\n",
        "Note: The preprocessing for this model is meant for natural language generation tasks. If you want a QA type instead we would take into account 2 outputs. Both over the same vocabulary space (context). If used for sentiment analysis our output space would be all available sentiments and we simply adjust the last dense layer in the 'KennethsCustomTransformer' class to have as many units as outputs. This is nice to work on because it makes me feel aware of how to implement a lot of these systems that I just see in research papers. "
      ],
      "metadata": {
        "id": "MGQOhPq59-UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf \n",
        "class KensReallyWeirdLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(KensReallyWeirdLayer, self).__init__()\n",
        "    self.units = units\n",
        "  def build(self, input_shape):\n",
        "    w_init_method = tf.random_normal_initializer()\n",
        "    b_init_method = tf.zeros_initializer()\n",
        "    self.w = tf.Variable(initial_value = w_init_method(shape=(input_shape[-1], self.units), dtype='float32'), trainable=True)\n",
        "    self.b = tf.Variable(initial_value = b_init_method(shape=(self.units,), dtype='float32'), trainable=True)\n",
        "  def call(self, input):\n",
        "    relu = tf.math.maximum(tf.matmul(input*self.w) + b, 0)\n",
        "    simoidfollowingrelu = 1/(1+tf.math.pow(tf.cast(e, dtype='float32'),tf.cast(-relu, dtype='float32')))\n",
        "    hyptangentofemall = tf.math.tanh(sigmoidfollowingrelu)\n",
        "    return hyptangentofemall "
      ],
      "metadata": {
        "id": "Px4AYi9Tu_JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "def internal_value(pos, i, d_model):\n",
        "  return pos/np.power(10000, (2*i)/d_model)\n",
        "def positional_encoding(max_position, d_model):\n",
        "  #Recall the formula is trig(pos/(10000^(2i/d_model)))\n",
        "  values = internal_value(np.arange(max_position)[:, np.newaxis], np.arange(d_model)[np.newaxis,:], d_model)\n",
        "  values[:, 0::2] = np.sin(values[:, 0::2])\n",
        "  values[:, 1::2] = np.cos(values[:, 0::2])\n",
        "  return values"
      ],
      "metadata": {
        "id": "9DdVwwUPyBbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_attention(value, key, query, dimensionality, mask):\n",
        "  dotproductkq = tf.matmul(key, query, transpose_b = True)\n",
        "  dotproductkq = dotproductkq/(tf.math.sqrt(dimensionality))\n",
        "  dotproductkq += (mask * -1e12) \n",
        "  softout = tf.nn.softmax(dotproductkq, axis = -1)\n",
        "  attout = tf.matmul(softout, value, transpose_b=True)\n",
        "  return attout"
      ],
      "metadata": {
        "id": "A4rDYxBUXO5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,dimensionality, num_heads, batch_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.dimensionality = dimensionality \n",
        "    self.num_heads = num_heads\n",
        "    assert self.dimensionality % self.num_heads == 0 \n",
        "    self.qW = tf.keras.layers.Dense(dimensionality)\n",
        "    self.kW = tf.keras.layers.Dense(dimensionality)\n",
        "    self.vW = tf.keras.layers.Dense(dimensionality)\n",
        "    #Now we have the matrices we are going to learn that is associated with each of these query, key, value components\n",
        "    self.dense = tf.keras.layers.Dense(dimensionality)\n",
        "    self.depth = self.dimensionality // self.num_heads\n",
        "  def call(self,query, key, value, mask):\n",
        "    q = self.qW(query)\n",
        "    k = self.kW(key)\n",
        "    v = self.vW(value)\n",
        "\n",
        "    q = tf.reshape(q, (batch_size, -1, self.num_heads, self.depth))\n",
        "    q = tf.transpose(q, perm = [0, 2,1,3])\n",
        "    v = tf.reshape(v, (batch_size, -1, self.num_heads, self.depth))\n",
        "    v = tf.transpose(v, perm = [0,2,1,3])\n",
        "    k = tf.reshape(k, (batch_size, -1, self.num_heads, self.depth))\n",
        "    k = tf.transpose(k, perm = [0,2,1,3])\n",
        "    attention_output = dot_product_attention(v, k, q, dimensionality, mask)\n",
        "    attention_output = tf.tranpose(attention_output, perm = [0,2,1,3])\n",
        "    attention_output = tf.reshape(attention_output, (batch_size, -1, self.dimensionality))\n",
        "    output = self.dense(attention_output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "0y4K6NM7g-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WEIRDLayers(dff, d_model):\n",
        "  return tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(dff, return_sequences=True),\n",
        "        tf.keras.layers.Dense(dff, activation = 'relu'),\n",
        "        tf.keras.layers.LeakyReLU(alpha = 0.5),\n",
        "        tf.keras.layers.LayerNormalization(epsilon = 1e-7),\n",
        "        tf.keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "metadata": {
        "id": "Cldw5aVnefoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we have an LSTM Layer, along with a FC Layer, into a LeakyRELU layer, into LayerNormalization into a final FC Layer.\n",
        "#Now we can build the EncoderLayer and DecoderLayer\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, batch_size):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads \n",
        "    self.mha= MultiHeadAttention(self.d_model, self.num_heads, batch_size)\n",
        "    self.weirdlayer1 = WEIRDLayers(dff, self.d_model)\n",
        "    self.weirdlayer2 = WEIRDLayers(dff, self.d_model)\n",
        "  def call(self, input, mask):\n",
        "    out1_1 = self.mha(input,input,input, mask)\n",
        "    out1_2 = tf.keras.layers.Dropout(0.3, training=True)(out1_1)\n",
        "    out1_3= tf.keras.layers.LayerNormalization(out1_2 + input)\n",
        "    out2_1 = self.weirdlayer1(out1_3)\n",
        "    out2_2 = tf.keras.layers.Dropout(0.3, training=True)(out2_1)\n",
        "    out2_3 = tf.keras.layers.LayerNormalization(out2_2 + out1_3)\n",
        "    out3 = self.weirdlayer2(out2_3)\n",
        "    return out3"
      ],
      "metadata": {
        "id": "gkCjUbW2lN8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeirdDecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, batch_size):\n",
        "    super(WeirdDecoderLayer, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.num_heads = num_heads\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads, batch_size)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads, batch_size)\n",
        "    self.weirdlayer1 = WEIRDLayers(dff, d_model)\n",
        "    self.weirdlayer2 = WEIRDLayers(dff, d_model)\n",
        "    self.weirdlayer3 = WEIRDLayers(dff, d_model)\n",
        "    self.layernorm1= tf.keras.layers.LayerNormalization()\n",
        "    self.dense = tf.keras.layers.Dense(dff)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization()\n",
        "    self.dropout1 = tf.keras.layers.Dropout(0.4)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(0.3)\n",
        "    self.Kenthebeast123 = KennethsWeirdLayer(d_model)\n",
        "  def call(self, input, enc_input, mask_forward, zero_mask):\n",
        "    out1_1 = self.mha1(input, input, input, mask_forward)\n",
        "    out1_2 = self.layernorm1(out1_1 + input)\n",
        "    out1_3 = self.weirdlayer1(out1_2 + out1_1 + input)\n",
        "    out1_4 = self.layernorm2(out1_3 + out1_2 + out1_1)\n",
        "    out2_1 = self.mha2(out1_4, enc_input, enc_input, zero_mask, batch_size) \n",
        "    #(ABOVE)QUERY IS ALWAYS DECODER OUTPUT, WE USE KEY AND VALUE FROM ENCODER TO MAKE IT SEEM LIKE THE ENCODER IS A MEMORY STORAGE\n",
        "    out2_2 = self.dropout(out2_1)\n",
        "    out2_3 = self.dense(out2_2)\n",
        "    out2_4 = self.weirdlayer2(out2_3)\n",
        "    out2_5 = self.dropout2(out2_4)\n",
        "    out3 = self.weirdlayer3(out2_5 + out2_4+ out2_1)\n",
        "    out3Ken = self.Kenthebeast123(out3)\n",
        "    return out3"
      ],
      "metadata": {
        "id": "00_ciciGlhdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomEncoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_heads,num_layers, d_model, batch_size, dff, distinct_vocab_size, max_position):\n",
        "    super(CustomEncoder, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads,dff, batch_size) for _ in range(self.num_layers)]\n",
        "    self.embedder = tf.keras.layers.Embedding(distinct_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encodindg(max_position, d_model)\n",
        "  def call(self, input, mask):\n",
        "    x = self.embedder(input)\n",
        "    sequenceLength = tf.shape(input)[1]\n",
        "    x += self.pos_encoding[:, :sequenceLength, :]\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](input, mask)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UXr_-Alxw0RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDecoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, dff,batch_size, num_heads, distinct_vocab_size, max_position):\n",
        "    super(CustomDecoder, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    self.d_model = d_model\n",
        "    self.embedder = tf.keras.layers.Embedding(distinct_vocab_size, self.d_model)\n",
        "    self.dec_layers = [WeirdDecoderLayer(d_model, num_heads, dff, batch_size) for _ in range(self.num_layers)]\n",
        "    self.pos_encoding = positional_encoding(max_position, d_model)\n",
        "  def call(self, input, enc_output, look_ahead_mask, zero_mask):\n",
        "    x = self.embedder(input)\n",
        "    x += pos_encoding[:, :tf.shape(input)[1], :]\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.dec_layers[i](input, enc_output, look_ahead_mask, zero_mask)\n",
        "    return x "
      ],
      "metadata": {
        "id": "_CB_A0F_5meU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KennethsCustomTransformer(tf.keras.Model):\n",
        "  def __init__(self, dec_num_layers, enc_num_layers, d_model, dff, num_heads, max_position, input_vocab_size,output_vocab_size, batch_size, dec_zero_mask, enc_zero_mask\n",
        "               ,dec_look_ahead_mask):\n",
        "    super(KennethsCustomTransformer, self).__init__()\n",
        "    self.encoder = CustomEncoder(num_heads, enc_num_layers, d_model, batch_size, dff, distinct_vocab_size, max_position)\n",
        "    self.decoder = CustomDecoder(dec_num_layers, d_model, dff, batch_size, num_heads, distinct_vocab_size, max_position)\n",
        "    self.dense = tf.keras.layers.Dense(output_vocab_size) #The amount of units is equivalent to the amount of outputs we are considering at each step.\n",
        "    #The amount of different dense layers corresponds to the amount of different output's we want. If this was question answering we'd have two dense layers that we can connect\n",
        "    #together. However here we are assuming a text generation probability output that we choose from for the next word. So we will use one output and a option size equivalent\n",
        "    #to the entire output vocabulary size\n",
        "  def call(self, input, label):\n",
        "    enc_out = self.encoder(input,enc_zero_mask)\n",
        "    dec_out = self.decoder(label,enc_out,dec_look_ahead_mask,dec_zero_mask)\n",
        "    finalout = self.dense(dec_out)\n",
        "    return finalout"
      ],
      "metadata": {
        "id": "Aw7fMzHP6Kl9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now to create an instance we simply have to create the hyperparameters, as well as regular parameters and then fit to our data!\n",
        "#The preprocessing step is pretty simple (usually) so once you have that setup all you have to do is input! We also need to take into account making the masks with the input\n",
        "#we have. We could incorporate it straight into the model but im a bit lazy and I have a lot of other work to do for school unfortunately.\n",
        "#Anyways this is my personally made weight transformer!"
      ],
      "metadata": {
        "id": "CJiuhJkU9QKQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Made this to show comfortability again with making advanced models. I added randomness to show this. I will be using this model to process some data and labels a bit later on but this is just the start. Next up on my list I believe I will be making different transformer variations and testing them to see which is the best performing."
      ],
      "metadata": {
        "id": "Us39DNSJ9sWd"
      }
    }
  ]
}