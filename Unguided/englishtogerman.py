# -*- coding: utf-8 -*-
"""EnglishtoGerman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A9yTdeyM7LJJdQBEwxs4kSjlrYa3eK2s
"""

import pandas as pd

EnglishGermanData = pd.read_csv("deu.txt", sep = '\t')

EnglishGermanData['Go.']

EnglishGermanData['Geh.']

import tensorflow as tf 
TfEnglishGerman = tf.data.Dataset.from_tensor_slices((EnglishGermanData['Go.'], EnglishGermanData['Geh.']))

for p in TfEnglishGerman.take(2):
  print(p[0].numpy().decode("utf-8"), p[1].numpy().decode("utf-8"))

print(len(TfEnglishGerman))

"""After inspecting the text file I was alone to see that we have around 250,000 instances in our data. It would not hurt to only use 1/5 of this, so lets just grab that many."""

SampledTFEnglishGerman = TfEnglishGerman.take(50000)
print(len(SampledTFEnglishGerman))

for j in SampledTFEnglishGerman.take(20):
  print(j[0].numpy().decode("utf-8"), j[1].numpy().decode("utf-8"))

#Im curious to know if this method takes the first 50000 instances or if it takes instances at random? The docs isnt very clear so lets see if first 20 instances in original dataset
#match the first twenty instancs in this dataset. Upon looking at the dataset I notice a bulk of the text is in the last thousands of instances. So intead lets grab from the last approx 50,000 instances

for j in TfEnglishGerman.take(20):
  print(j[0].numpy().decode("utf-8"), j[1].numpy().decode("utf-8"))

TrainingENDEDATA = TfEnglishGerman.skip(200000) #Grabs last 49,229 instances

print(len(TrainingENDEDATA))

for z in TrainingENDEDATA.take(10):
  print(z[0].numpy().decode("utf-8"), z[1].numpy().decode("utf-8"))

"""Since we are going to be making a dynamical model, we dont really need to fix the size of the data to a certain token length, they can all vary. However lets just check to see the length of the longest sentence to see what the max length is."""

englishmax = 0
import numpy as np 
for obj in TrainingENDEDATA.take(len(TrainingENDEDATA)):
  if len(obj[0].numpy().decode("utf-8").split())>englishmax:
    englishmax = len(obj[0].numpy().decode("utf-8").split())
germanmax = 0
for obj1 in TrainingENDEDATA.take(len(TrainingENDEDATA)):
  if len(obj[1].numpy().decode("utf-8").split())> germanmax:
    germanmax = len(obj[1].numpy().decode("utf-8").split())

print(englishmax, germanmax) #Longest sequence of words in english data is 101 words, and in the german its 76 words. so tbh I want to tokenize these and pad to length 128

cleanedenglish = []
cleanedgerman = []
for vals in TrainingENDEDATA.take(len(TrainingENDEDATA)):
  cleanedenglish.append(vals[0].numpy().decode("utf-8"))
  cleanedgerman.append(vals[1].numpy().decode("utf-8"))
#Now after we grab the above lets remove anything thats not a a-Z letter and not a number between 0-9 
import re 
cleanedenglish = [re.sub(r'[^a-zA-Z0-9]', ' ', entext) for entext in cleanedenglish]
cleanedgerman = [re.sub(r'[^a-zA-Z0-9]', ' ', detext) for detext in cleanedgerman]
#Now that this is cleaned lets put it into a dataset and prepare the tokenizer.
CleanedENDEDATA = tf.data.Dataset.from_tensor_slices((cleanedenglish, cleanedgerman))

"""## Starting the TextVectorization Process"""

npenglish = np.array(cleanedenglish)
npgerman = np.array(cleanedgerman)
unique_en_words = len(set(npenglish.flatten()))
unique_german_words = len(set(npgerman.flatten()))
print(unique_en_words, unique_german_words)

endummydata = tf.data.Dataset.from_tensor_slices(cleanedenglish)
dedummydata = tf.data.Dataset.from_tensor_slices(cleanedgerman)
enTextVectorizer = tf.keras.layers.TextVectorization(max_tokens = unique_en_words, standardize = 'lower_and_strip_punctuation', output_mode='int', output_sequence_length=128,
                                                     pad_to_max_tokens=True)
deTextVectorizer = tf.keras.layers.TextVectorization(max_tokens = unique_german_words, standardize='lower_and_strip_punctuation', output_mode='int', output_sequence_length=128,
                                                     pad_to_max_tokens=True)
enTextVectorizer.adapt(endummydata)
deTextVectorizer.adapt(dedummydata)

def TokenizeDataset(english, german):
  return enTextVectorizer(english), deTextVectorizer(german)
CleanedENDEDATA = CleanedENDEDATA.cache().shuffle(len(CleanedENDEDATA)).batch(64).map(TokenizeDataset, num_parallel_calls = tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)

for tokenized in CleanedENDEDATA.take(1):
  print(tokenized[0], tokenized[1]) #Fair enough! So lets now take a look to make sure the length is still proper, it should be originallength/64

len(CleanedENDEDATA)

print(49229/64) #Lets remove the last batch! since they rounded up.

CleanedENDEDATA = CleanedENDEDATA.take(769)

len(CleanedENDEDATA)

#Our data seems to be ready, its in tuple form, batches, parallelized, and prepped.
#Time to make our model.
def point_wise_feed_forward_network(dff, d_model):
  return tf.keras.Sequential([
          tf.keras.layers.Dense(dff, activation = 'relu'),
          tf.keras.layers.Dense(d_model) 
  ])

def scaled_dot_product_attention(q, v, k, mask):
  scores = tf.linalg.matmul(q,k, transpose_b=True)
  scaledscores = scores/tf.math.sqrt(tf.cast((tf.shape(v)[-1]), tf.float32))
  if mask is not None:
    scaledscores += (mask * -1e9)
  softmaxscores = tf.nn.softmax(scaledscores)
  return tf.matmul(softmaxscores, v), softmaxscores

"""## Positional Encoding
## Formula: cos for odds, sin for even pos/(10000^((2*i)/d_model))

"""

def innerpart(position, iofd, d_model):
  return position/(np.power(10000, (2*iofd)/d_model))
def positional_encoding(maximum_position, d_model):
  untrigedarr = innerpart(np.arange(maximum_position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
  untrigedarr[:, 0::2] = np.sin(untrigedarr[:, 0::2])
  untrigedarr[:, 1::2] = np.cos(untrigedarr[:, 1::2])
  return tf.cast(untrigedarr[np.newaxis, ... ], tf.float32) #We raise the dimension for the sake of tensorflow, as the second index refers to rows and the third refers to columns.
  #The first index refers to batch index.

def create_look_ahead_mask(size):
  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
  return look_ahead_mask
def create_padding_mask(sequence):
  values = tf.cast(tf.math.equal(sequence, 0), tf.float32)
  return values[:, tf.newaxis, tf.newaxis, :]

class MultiHeadAttention(tf.keras.layers.Layer):
  def __init__(self, num_heads, d_model):
    super(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model 
    assert self.d_model % self.num_heads == 0 #Throws an error if the dimensionality of our vector embeddings are not divisible by the number of heads we select. 
    self.depth = self.d_model//self.num_heads 
    self.wQ = tf.keras.layers.Dense(d_model)
    self.wK = tf.keras.layers.Dense(d_model)
    self.wV = tf.keras.layers.Dense(d_model)
    self.finaloutput = tf.keras.layers.Dense(d_model)
  def reshape(self, x, batch_size):
    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
    x = tf.transpose(x, perm = [0,2,1,3]) #Goes from batch -> seq_len->num_heads->depth to batch->num_heads -> seq_len -> depth
    return x
  def call(self, query, key, value, mask):
    batch_size = tf.shape(query)[0]
    q = self.wQ(query)
    k = self.wK(key)
    v = self.wV(value)
    q = self.reshape(q, batch_size)
    v = self.reshape(v, batch_size)
    k = self.reshape(k, batch_size) 
    attentionout, attentionweights = scaled_dot_product_attention(q, v, k, mask)
    attentionout = tf.transpose(attentionout, perm=[0,2,1,3])
    attentionout = tf.reshape(attentionout, (batch_size, -1, self.d_model)) #This concatenates the attention pieces again.
    linearout = self.finaloutput(attentionout)
    return linearout, attentionweights

class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self, num_heads, d_model, dff, rate = 0.05):
    super(EncoderLayer, self).__init__() 
    self.d_model = d_model 
    self.num_heads = num_heads
    self.ffn = point_wise_feed_forward_network(dff, d_model)
    self.mha= MultiHeadAttention(self.num_heads, self.d_model)
    self.layernorm1 = tf.keras.layers.LayerNormalization()
    self.layernorm2 = tf.keras.layers.LayerNormalization()
    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)
  def call(self, input,Training,  padding_mask):
    x, _ = self.mha(input, input, input, padding_mask)
    x = self.dropout1(x, training = Training)
    x = self.layernorm1(x + input)
    ffn = self.ffn(x)
    ffn = self.dropout2(ffn, training = Training)
    ffn = self.layernorm2(ffn + x)
    return ffn

sample_encoder_layer = EncoderLayer(8,512, 2048)

sample_encoder_layer_output = sample_encoder_layer(
    tf.random.uniform((64, 43, 512)), False, None)

sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)

class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads, dff, rate = 0.05):
    super(DecoderLayer, self).__init__()
    self.d_model = d_model
    self.num_heads = num_heads 
    self.ffn = point_wise_feed_forward_network(dff, d_model)
    self.mha1 = MultiHeadAttention(num_heads, d_model)
    self.mha2 = MultiHeadAttention(num_heads, d_model)
    self.ln1 = tf.keras.layers.LayerNormalization()
    self.ln2 = tf.keras.layers.LayerNormalization()
    self.ln3 = tf.keras.layers.LayerNormalization()
    self.do1 = tf.keras.layers.Dropout(rate)
    self.do2 = tf.keras.layers.Dropout(rate)
    self.do3 = tf.keras.layers.Dropout(rate)
  def call(self, input, enc_output,training, padding_mask, look_ahead_mask):
    x, attnw1 = self.mha1(input, input, input, look_ahead_mask)
    x = self.do1(x, training = training)
    x = self.ln1(x + input)
    x2,attnw2 = self.mha2(x, enc_output, enc_output, padding_mask)
    x2 = self.do2(x2, training = training)
    x2 = self.ln2(x2 + x)
    ffn1 = self.ffn(x2)
    ffn1 = self.do3(ffn1, training = training)
    ffn1 = self.ln3(ffn1 + x2)
    return ffn1, attnw1, attnw2

sample_decoder_layer = DecoderLayer(512, 8, 2048)

sample_decoder_layer_output, _, _ = sample_decoder_layer(
    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,
    False, None, None)

sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)

class Encoder(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads, num_layers, dff, input_vocab_size, maximum_position_encoding):
    super(Encoder, self).__init__()
    self.num_layers = num_layers 
    self.d_model = d_model 
    self.enc_layers = [EncoderLayer(num_heads, d_model, dff) for _ in range(num_layers)]
    self.embedding = tf.keras.layers.Embedding(input_dim = input_vocab_size, output_dim = d_model)
    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
    self.dropout = tf.keras.layers.Dropout(0.1)
  def call(self, input, training, mask):
    seq_len = tf.shape(input)[1] 
    embeddedin = self.embedding(input)
    embeddedin *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #Check if this is required too
    embeddedin += self.pos_encoding[:, :seq_len, :] #Since we have all sequences to length 128 we can check after if this argument is really necessary to get a valid training in.
    embeddedin = self.dropout(embeddedin)
    output = embeddedin 
    for z in range(self.num_layers):
      output = self.enc_layers[z](output, training, padding_mask=mask)
    return output

sample_encoder = Encoder(d_model=512, num_heads=8, num_layers=2,
                         dff=2048, input_vocab_size=8500,
                         maximum_position_encoding=10000)
temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)

sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)

print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)

class Decoder(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads, num_layers, dff, target_vocab_size, maximum_position_encoding):
    super(Decoder, self).__init__()
    self.d_model = d_model 
    self.num_heads = num_heads
    self.num_layers = num_layers
    self.dec_layers = [DecoderLayer(d_model, num_heads, dff) for _ in range(self.num_layers)]
    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)
    self.dropout1 = tf.keras.layers.Dropout(0.1)
    self.embedding = tf.keras.layers.Embedding(input_dim = target_vocab_size, output_dim = d_model)
  def call(self, input, enc_output, training, padding_mask, look_ahead_mask):
    seq_len = tf.shape(input)[1]
    x = self.embedding(input)
    x *= tf.math.sqrt(tf.cast(self.d_model ,tf.float32))
    attentionlist = []
    x += self.pos_encoding[:, :seq_len, :]
    for z in range(self.num_layers):
      x, attnb1, attnb2 = self.dec_layers[z](x, enc_output, training, padding_mask, look_ahead_mask)
      attentionlist.append(attnb1)
      attentionlist.append(attnb2)
    return x, attentionlist

sample_decoder = Decoder(d_model=512, num_heads=8,num_layers=2,
                         dff=2048, target_vocab_size=8000,
                         maximum_position_encoding=5000)
temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)

output, attn = sample_decoder(temp_input,
                              enc_output=sample_encoder_output,
                              training=False,
                              look_ahead_mask=None,
                              padding_mask=None)
output.shape

class Transformer(tf.keras.Model):
  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate = 0.1):
    super().__init__()
    self.encoder = Encoder(d_model, num_heads, num_layers, dff, input_vocab_size, pe_input)
    self.decoder = Decoder(d_model, num_heads, num_layers, dff, target_vocab_size, pe_target)
    self.final_layer = tf.keras.layers.Dense(target_vocab_size) #Since we are predicing over all german words
  def call(self, inputs, training):
    x, y = inputs  #Since we have a tuple as our dataset
    enc_pad_mask, dec_look_ahead_mask, dec_padding_mask = self.create_masks(x, y)
    enc_out = self.encoder(x, training, enc_pad_mask)
    dec_out, attn = self.decoder(y, enc_out, training, dec_padding_mask, dec_look_ahead_mask)
    finaloutput = self.final_layer(dec_out)
    return finaloutput, attn
  def create_masks(self, inp, tar):
    enc_padding_mask = create_padding_mask(inp)
    dec_padding_mask = create_padding_mask(inp) #Second attention block in decoder
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)
    return enc_padding_mask, look_ahead_mask, dec_padding_mask

sample_transformer = Transformer(
    num_layers=2, d_model=512, num_heads=8, dff=2048,
    input_vocab_size=8500, target_vocab_size=8000,
    pe_input=10000, pe_target=6000)

temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)
temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)

fn_out, _ = sample_transformer([temp_input, temp_target], training=False)

fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)

num_layers = 4
d_model = 128
dff = 512
num_heads = 8
dropout_rate = 0.1

class LearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super(LearningRateSchedule, self).__init__()
    self.d_model = d_model 
    self.d_model = tf.cast(self.d_model, tf.float32)
    self.warmup_steps =warmup_steps
  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)
    return tf.math.rsqrt(self.d_model)*tf.math.minimum(arg1,arg2)

optimizer = tf.keras.optimizers.Adam(learning_rate = LearningRateSchedule(d_model), epsilon = 1e-9)

temp_learning_rate_schedule = LearningRateSchedule(d_model)
import matplotlib.pyplot as plt
plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))
plt.ylabel("Learning Rate")
plt.xlabel("Train Step")

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')

def loss_func(real, pred):
  diff = loss(real, pred)
  return tf.reduce_sum(diff)
train_loss = tf.keras.metrics.Mean(name='train_loss')

transformer1 = Transformer(num_layers=num_layers, d_model = d_model, num_heads = num_heads, dff= dff, input_vocab_size = unique_en_words, target_vocab_size = unique_german_words,
                           pe_input= 1000, pe_target=1000)

checkpoint_path = "./checkpoints/train"

ckpt = tf.train.Checkpoint(transformer=transformer1,
                           optimizer=optimizer)

ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)

# if a checkpoint exists, restore the latest checkpoint.
if ckpt_manager.latest_checkpoint:
  ckpt.restore(ckpt_manager.latest_checkpoint)
  print('Latest checkpoint restored!!')

EPOCHS = 20

def train_step(inp, tar):
  tar_inp = tar[:, :-1]
  tar_real = tar[:, 1:]

  with tf.GradientTape() as tape:
    predictions, _ = transformer1([inp, tar_inp],
                                 training = True)
    loss = loss_func(tar_real, predictions)

  gradients = tape.gradient(loss, transformer1.trainable_variables)
  optimizer.apply_gradients(zip(gradients, transformer1.trainable_variables))

  train_loss(loss)

!pip install tensorflow_text

import collections
import logging
import os
import pathlib
import re
import string
import sys
import time

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
import tensorflow_text as text
import tensorflow as tf

for epoch in range(EPOCHS):
  start = time.time()

  train_loss.reset_states()

  # inp -> portuguese, tar -> english
  for (batch, (inp, tar)) in enumerate(CleanedENDEDATA):
    train_step(inp, tar)

    if batch % 50 == 0:
      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f}')

  if (epoch + 1) % 5 == 0:
    ckpt_save_path = ckpt_manager.save()
    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')

  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f}')

  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\n')

!mkdir -p saved_model
transformer1.save('saved_model/my_transformer')

