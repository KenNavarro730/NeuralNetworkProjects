{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VNYAXBWtTfz",
        "outputId": "9c5b8266-14d9-4444-8f18-f2077c83cc23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  586k  100  586k    0     0  1995k      0 --:--:-- --:--:-- --:--:-- 1995k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://s3.amazonaws.com/text-datasets/nietzsche.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxpC8jvKtT-Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "batch_size = 64 \n",
        "raw_data_ds = tf.data.TextLineDataset([\"nietzsche.txt\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iJKKXmFtkns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c4d17e-0d49-4355-996f-f0a92ea7b3ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE\n",
            "\n",
            "\n",
            "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
            "for suspecting that all philosophers, in so far as they have been\n",
            "dogmatists, have failed to understand women--that the terrible\n",
            "seriousness and clumsy importunity with which they have usually paid\n",
            "their addresses to Truth, have been unskilled and unseemly methods for\n",
            "winning a woman? Certainly she has never allowed herself to be won; and\n",
            "at present every kind of dogma stands with sad and discouraged mien--IF,\n"
          ]
        }
      ],
      "source": [
        "for elem in raw_data_ds.take(10):\n",
        "  print(elem.numpy().decode(\"utf-8\")) # before we can decode to regular text we need to convert it to numpy\n",
        "#Now that we can see the text exists, what we need to do is convert all of this text into one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cBsvbqOtr5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f12a4fc-3ea1-46f3-bb3b-10c6e5821cb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(100,), dtype=string, numpy=\n",
              "array([b'P', b'R', b'E', b'F', b'A', b'C', b'E', b'S', b'U', b'P', b'P',\n",
              "       b'O', b'S', b'I', b'N', b'G', b' ', b't', b'h', b'a', b't', b' ',\n",
              "       b'T', b'r', b'u', b't', b'h', b' ', b'i', b's', b' ', b'a', b' ',\n",
              "       b'w', b'o', b'm', b'a', b'n', b'-', b'-', b'w', b'h', b'a', b't',\n",
              "       b' ', b't', b'h', b'e', b'n', b'?', b' ', b'I', b's', b' ', b't',\n",
              "       b'h', b'e', b'r', b'e', b' ', b'n', b'o', b't', b' ', b'g', b'r',\n",
              "       b'o', b'u', b'n', b'd', b'f', b'o', b'r', b' ', b's', b'u', b's',\n",
              "       b'p', b'e', b'c', b't', b'i', b'n', b'g', b' ', b't', b'h', b'a',\n",
              "       b't', b' ', b'a', b'l', b'l', b' ', b'p', b'h', b'i', b'l', b'o',\n",
              "       b's'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "text = \"\"\n",
        "for lines in raw_data_ds:\n",
        "  text += lines.numpy().decode(\"utf-8\")\n",
        "splitted = tf.strings.bytes_split(text)\n",
        "splitted[:100] #Note that here we are doing character level tokenization.\n",
        "#It is general advice to have at least 100,000 characters in our training corpus, 1M is even better so lets check how many characters we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CKUmzgtvUMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fa089f8-53fc-444e-b86e-9b6525074ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "83\n"
          ]
        }
      ],
      "source": [
        "#The number of distinct charaters\n",
        "vals = list(set(text))\n",
        "print(len(vals)) #We have 83 distinct characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTsMm2KhvjxZ"
      },
      "outputs": [],
      "source": [
        "#Now lets split our data into two parts, lets have the data part and the label part. The data will be a sequence that has a fixed max length\n",
        "#the label will be the next character produced by this sequence for each sequence.\n",
        "#input_chars -> model -> next_char (and input_chars have max_length)\n",
        "maxlen = 20 #Longest input sequence of characters we predict on \n",
        "step = 3\n",
        "input_chars = []\n",
        "next_chars= []\n",
        "#Since we are going to for loop the step lets us avoid reusing sequences that only differ by two characters at a time, instead we differ by\n",
        "#losing 3 and gaining 3 new ones which is 6 in total difference\n",
        "for i in range(0, len(text)-maxlen, step):\n",
        "  input_chars.append(text[i:i+maxlen])\n",
        "  next_chars.append(text[i+maxlen]) #We set the ending spot to be len(text)-maxlen since we are indexing + maxlen everytime so if we went to\n",
        "  #the full length of the text with i, and we tried to index i + maxlen, this would be trying to index out of bounds by an amount of maxlen\n",
        "  #since i at that point is already max len\n",
        "#Lets check to make sure the first couple of instances worked\n",
        "# print(input_chars[6], next_chars[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGMCVy4ZxWn0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4965745-8ce7-4f07-c08b-6749511bdb84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Truth is a woman--w h\n"
          ]
        }
      ],
      "source": [
        "print(input_chars[7], next_chars[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4QB0RVfxiI8"
      },
      "outputs": [],
      "source": [
        "#Now we can make this into datasets! Since we are loading from memory we use .from_tensor_slices\n",
        "X_raw_train_ds = tf.data.Dataset.from_tensor_slices(input_chars)\n",
        "Y_raw_train_ds = tf.data.Dataset.from_tensor_slices(next_chars)\n",
        "#Now we can still view this using a for loop and zip() and numpy() and decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV0xNNaHyZw5"
      },
      "outputs": [],
      "source": [
        "#Now we have to do some preprocessing\n",
        "#1- Standardizing (lower casing and removing punctuations)\n",
        "#2- Split each sample into word length\n",
        "#3- Recombine substrings into tokens (1-gram here for character length)\n",
        "#4- Index tokens\n",
        "#5- Transform each of these index tokens into vector representations (Embedding) (A vector of ints or dense float vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tde-n407zl-u"
      },
      "outputs": [],
      "source": [
        "import re \n",
        "import string \n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
        "  stripped_num = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n",
        "  stripped_punc = tf.strings.regex_replace(stripped_num, \"[%s]\" % re.escape(string.punctuation), \" \")\n",
        "  return stripped_punc \n",
        "def char_split(input_data):\n",
        "  return tf.strings.unicode_split(input_data, 'UTF-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVyAiMaG0QP6"
      },
      "outputs": [],
      "source": [
        "#We are able to limit the number of distinct characters in TextVectorization we also set fixed size seq length which is required by our model\n",
        "max_features = 83 #number of distinct word\n",
        "embedding_dim = 16 #embedding layer output dimension\n",
        "seq_length = maxlen #size of input to model, but size of output of embeddings per sequence we feed it! \n",
        "#Note that we will specify whether we are doing character split or word split in our TextVectorization function.\n",
        "#tf.squeeze removes all of the 1 dimension parts of an input if our inputs dim is (1,2,3,1) it becomes (2,3)\n",
        "#Here what we will do in text vectorization is normalize, split, and then set the output to be integers representing the characters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize =custom_standardization,\n",
        "    max_tokens = max_features,\n",
        "    split = char_split,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = seq_length\n",
        ")\n",
        "#No matter size of input the output embedding size will be of size 20, however  we can have 83 distinct characters and these output embeddings\n",
        "#will be integers. For the y dataset we will be taking the first numerical indice in the embeddings to represent one character rather than \n",
        "#use a whole sequence type embedding to represent a single character.\n",
        "vectorize_layer.adapt(X_raw_train_ds)"
      ],
      "metadata": {
        "id": "0tiGiJ7WHyDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_vectorize(text): #the point of expanding dim is to raise from a regular string to an array which we can perform textvectorization on\n",
        "  value = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(value))\n",
        "X_train_ds = X_raw_train_ds.map(text_vectorize)\n",
        "Y_train_ds = Y_raw_train_ds.map(text_vectorize)\n",
        "Y_train_ds = Y_train_ds.map(lambda x: x[0])\n",
        "train_ds = tf.data.Dataset.zip((X_train_ds, Y_train_ds)).shuffle(buffer_size=512).batch(batch_size,drop_remainder=True).cache().prefetch(\n",
        "    buffer_size=tf.data.AUTOTUNE\n",
        ")"
      ],
      "metadata": {
        "id": "3eCprm0tJdG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are going to work on the sampling methods \n",
        "def softmax(z):\n",
        "  return np.exp(z)/sum(np.exp(z))\n",
        "def greedy_search(conditional_probability):\n",
        "  return np.argmax(conditional_probability)\n",
        "def temperature_sampling(conditional_probability, temperature=1.0):\n",
        "  conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n",
        "  conditional_probability = np.log(conditional_probability)/temperature \n",
        "  reweighted_conditional_probability = softmax(conditional_probability)\n",
        "  probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n",
        "  return np.argmax(probas)\n",
        "def top_k_sampling(conditional_probability, k):\n",
        "  top_k_probabilities, top_k_indices = tf.math.top_k(conditional_probability, k=k, sorted=True)\n",
        "  top_k_probabilities = np.asarray(top_k_probabilities).astype(\"float32\")\n",
        "  top_k_probabilities = np.squeeze(top_k_probabilities)\n",
        "  top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n",
        "  top_k_redistributed_probabilities = softmax(top_k_probabilities)\n",
        "  top_k_redistributed_probabilities = np.asarray(top_k_redistributed_probabilities).astype(\"float32\")\n",
        "  return np.random.choice(np.squeeze(top_k_indices), p = top_k_redistributed_probabilities)\n",
        "#these all simply return indice of the given conditional probability and extra arguments"
      ],
      "metadata": {
        "id": "8B0k9VjfIE8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we simply have to build the language model, remember that our tokens were of length 20 for character level\n",
        "inputs = tf.keras.Input(shape=(20), dtype = 'int64')\n",
        "x = tf.keras.layers.Embedding(max_features, embedding_dim)(inputs) #Takes token in to make embeddings \n",
        "#Embedding needs to know # of distinct tokens and #of output dim \"embedding_dim\"\n",
        "#The embedder wants to know the number of distinct tokens \n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.LSTM(128, return_sequences=True)(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "predictions = tf.keras.layers.Dense(max_features, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = predictions)\n",
        "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "yF_wWDMBMsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhdlhiP-M0qr",
        "outputId": "cc80c8a2-fecd-4531-9fa1-392c5a2b2b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20)]              0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 20, 16)            1328      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 20, 16)            0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 20, 128)           74240     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 2560)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 83)                212563    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 288,131\n",
            "Trainable params: 288,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, epochs = 11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEXnOFFAM2fJ",
        "outputId": "9357b237-2c6f-4b8f-f21d-d4777cbc4ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/11\n",
            "3077/3077 [==============================] - 17s 5ms/step - loss: 2.0019 - accuracy: 0.4013\n",
            "Epoch 2/11\n",
            "3077/3077 [==============================] - 17s 5ms/step - loss: 1.9294 - accuracy: 0.4230\n",
            "Epoch 3/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.8726 - accuracy: 0.4395\n",
            "Epoch 4/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.8283 - accuracy: 0.4524\n",
            "Epoch 5/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.7906 - accuracy: 0.4633\n",
            "Epoch 6/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.7572 - accuracy: 0.4724\n",
            "Epoch 7/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.7301 - accuracy: 0.4799\n",
            "Epoch 8/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.7060 - accuracy: 0.4868\n",
            "Epoch 9/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.6866 - accuracy: 0.4930\n",
            "Epoch 10/11\n",
            "3077/3077 [==============================] - 16s 5ms/step - loss: 1.6686 - accuracy: 0.4976\n",
            "Epoch 11/11\n",
            "3077/3077 [==============================] - 17s 5ms/step - loss: 1.6523 - accuracy: 0.5027\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a1dfcb090>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now its pretty simple to do the sampling, all we have to do is pass in our models prediction into the functions we made above, and then take that returned index\n",
        "#and use .get_vocabulary()[returned index] #This is what we need to do in order to get back our models predictions. For some reason my code fails to exectue when I try to do it\n",
        "#as it is saying there is something wrong with my javascript. Either way here is the really basic but pretty trash model. Could add more LSTM layers and distribute the units\n",
        "#accross them. However this is a pretty weak model and I am going to see how I can improve this model drastically. Note that also this corpus is not good enough to produce\n"
      ],
      "metadata": {
        "id": "7jZRaXL-RAIJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TensorflowCharacterLevelNLG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}