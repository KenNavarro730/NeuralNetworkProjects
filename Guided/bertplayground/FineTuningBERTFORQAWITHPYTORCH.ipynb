{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FineTuningBERTFORQAWITHPYTORCH.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os "
      ],
      "metadata": {
        "id": "paNv2Vv1f55g"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/'"
      ],
      "metadata": {
        "id": "EsojRZPwgK59"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests"
      ],
      "metadata": {
        "id": "tc2KEAkDgVaP"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for file in ['train-v2.0.json', 'dev-v2.0.json']:\n",
        "  res = requests.get(f'{url}{file}')\n",
        "  #after make request we save file to squad\n",
        "  with open(f'squad/{file}', 'wb') as f: #wb for writing binary file\n",
        "    for chunk in res.iter_content(chunk_size=4):\n",
        "      f.write(chunk)\n",
        "    res"
      ],
      "metadata": {
        "id": "rOxa3cOsgbER"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Again what we are really training is answer start and answer end, in the train file we have question, context, answer as 'text', and answer start\n",
        "#defining the initial index\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3UpvWvUh0On",
        "outputId": "46fd447d-639d-49aa-b2a9-8d59157539bc"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Response [200]>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREP: All we want is the context, the questions, the answers! We are just gonna have a lists of stringest at the ends, we will also have the starting position."
      ],
      "metadata": {
        "id": "Y7EQRlztiOcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "pFB4HDXNhKFA"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('squad/train-v2.0.json', 'rb') as f:\n",
        "  squad_dict = json.load(f)"
      ],
      "metadata": {
        "id": "9WZa5J7QjCFT"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(squad_dict['data']) "
      ],
      "metadata": {
        "id": "gKWAsbxDAAxf",
        "outputId": "79a73c32-c8d4-456f-8bfb-117dbe15367e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "442"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squad_dict['data'][0]['paragraphs'][2]['context'] #as we can see from the top, all of the context info is within 'paragraphs'\n",
        "#Then here we can extract the context \n",
        "#Seems like its dictionary->list->dictionary->list->dictionary for the value we need!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "-o_ASMFLkIBh",
        "outputId": "f4de289f-0c18-4b6f-8419-1821391a5642"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A self-described \"modern-day feminist\", Beyonc√© creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny\\'s Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award\\'s history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.'"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#So its clear to grab the context we have to use a for loop!\n",
        "\n",
        "def read_data(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "    contexts = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "  for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "      context = passage['context']\n",
        "      for qa in passage['qas']:\n",
        "        question = qa['question']\n",
        "        if 'plausible_answers' in qa.keys():\n",
        "          access = 'plausible_answers'\n",
        "        else:\n",
        "          access = 'answers'\n",
        "        for answer in qa[access]:\n",
        "          contexts.append(context)\n",
        "          questions.append(question)\n",
        "          answers.append(answer)\n",
        "#We want to do this for both the training and validation set so lets insert this into a function.\n",
        "    return contexts, questions, answers"
      ],
      "metadata": {
        "id": "JhPx_e2Jkmcg"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_contexts, train_questions, train_answers = read_data('squad/train-v2.0.json')\n",
        "test_contexts, test_questions, test_answers = read_data('squad/dev-v2.0.json')"
      ],
      "metadata": {
        "id": "oF0WYJMzrz9j"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_answers[0])"
      ],
      "metadata": {
        "id": "Ac15B-mH-gqG",
        "outputId": "bbe58907-cc25-4626-bd1b-9bcb0ec5feb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_contexts[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us91whUetOZp",
        "outputId": "bf1e5e70-c00e-439f-dce4-6f7bffbda091"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.',\n",
              " 'The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse (\"Norman\" comes from \"Norseman\") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.']"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We have the starting positions, but we also need the end index\n",
        "train_answers[:2] #We cant actually just grab length of text and add that on to the starting index because some of the answer starts are incorrect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoyvIbFttQG-",
        "outputId": "6f693fec-d9e2-4969-c9c1-c4c5ac3f5a07"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer_start': 269, 'text': 'in the late 1990s'},\n",
              " {'answer_start': 207, 'text': 'singing and dancing'}]"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_end_index(answers, contexts):\n",
        "  for answer,context in zip(answers, contexts):\n",
        "    gold_text = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    end_idx = start_idx + len(gold_text)\n",
        "    #Now we add in the logic where the characters index are off \n",
        "    if context[start_idx:end_idx] == gold_text:\n",
        "      answer['answer_end'] = end_idx \n",
        "    else:\n",
        "      for n in [1,2]:\n",
        "        if context[start_idx-n,end_idx-n] == gold_text:\n",
        "          answer['answer_start'] = context[start_idx-n]\n",
        "          answer['answer_end'] = context[end_idx - n]\n",
        "add_end_index(train_answers, train_contexts)\n",
        "add_end_index(test_answers, test_contexts)"
      ],
      "metadata": {
        "id": "SOlpYftwtZGQ"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_answers[0]['answer_end']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwGkq3QWu9Xs",
        "outputId": "c42be204-d60d-47c1-a91a-7c5ce0a67db9"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenize/Encode text"
      ],
      "metadata": {
        "id": "zPP9eyG9vYHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers \n",
        "from transformers import DistilBertTokenizerFast "
      ],
      "metadata": {
        "id": "bHvTmdNUvhPi"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "xuXmBkkovujr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
        "#This is is now going to merge the two strings together but there will be a seperator token \n",
        "#Truncation and padding set to true of course to make sure they are all of the same size \n",
        "test_encoding = tokenizer(test_contexts, test_questions, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "uVUjS606v6MF"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding.keys() #We have the input ids and attention masks!\n",
        "train_encoding['input_ids'][0] #As we can see the tokenizer swithced it from words to tokens to ids ! with a cls = 101, sep = 102\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qm8bB68wXU6",
        "outputId": "0efd6dba-0aad-4536-ea08-62061e82581a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101,\n",
              " 20773,\n",
              " 21025,\n",
              " 19358,\n",
              " 22815,\n",
              " 1011,\n",
              " 5708,\n",
              " 1006,\n",
              " 1013,\n",
              " 12170,\n",
              " 23432,\n",
              " 29715,\n",
              " 3501,\n",
              " 29678,\n",
              " 12325,\n",
              " 29685,\n",
              " 1013,\n",
              " 10506,\n",
              " 1011,\n",
              " 10930,\n",
              " 2078,\n",
              " 1011,\n",
              " 2360,\n",
              " 1007,\n",
              " 1006,\n",
              " 2141,\n",
              " 2244,\n",
              " 1018,\n",
              " 1010,\n",
              " 3261,\n",
              " 1007,\n",
              " 2003,\n",
              " 2019,\n",
              " 2137,\n",
              " 3220,\n",
              " 1010,\n",
              " 6009,\n",
              " 1010,\n",
              " 2501,\n",
              " 3135,\n",
              " 1998,\n",
              " 3883,\n",
              " 1012,\n",
              " 2141,\n",
              " 1998,\n",
              " 2992,\n",
              " 1999,\n",
              " 5395,\n",
              " 1010,\n",
              " 3146,\n",
              " 1010,\n",
              " 2016,\n",
              " 2864,\n",
              " 1999,\n",
              " 2536,\n",
              " 4823,\n",
              " 1998,\n",
              " 5613,\n",
              " 6479,\n",
              " 2004,\n",
              " 1037,\n",
              " 2775,\n",
              " 1010,\n",
              " 1998,\n",
              " 3123,\n",
              " 2000,\n",
              " 4476,\n",
              " 1999,\n",
              " 1996,\n",
              " 2397,\n",
              " 4134,\n",
              " 2004,\n",
              " 2599,\n",
              " 3220,\n",
              " 1997,\n",
              " 1054,\n",
              " 1004,\n",
              " 1038,\n",
              " 2611,\n",
              " 1011,\n",
              " 2177,\n",
              " 10461,\n",
              " 1005,\n",
              " 1055,\n",
              " 2775,\n",
              " 1012,\n",
              " 3266,\n",
              " 2011,\n",
              " 2014,\n",
              " 2269,\n",
              " 1010,\n",
              " 25436,\n",
              " 22815,\n",
              " 1010,\n",
              " 1996,\n",
              " 2177,\n",
              " 2150,\n",
              " 2028,\n",
              " 1997,\n",
              " 1996,\n",
              " 2088,\n",
              " 1005,\n",
              " 1055,\n",
              " 2190,\n",
              " 1011,\n",
              " 4855,\n",
              " 2611,\n",
              " 2967,\n",
              " 1997,\n",
              " 2035,\n",
              " 2051,\n",
              " 1012,\n",
              " 2037,\n",
              " 14221,\n",
              " 2387,\n",
              " 1996,\n",
              " 2713,\n",
              " 1997,\n",
              " 20773,\n",
              " 1005,\n",
              " 1055,\n",
              " 2834,\n",
              " 2201,\n",
              " 1010,\n",
              " 20754,\n",
              " 1999,\n",
              " 2293,\n",
              " 1006,\n",
              " 2494,\n",
              " 1007,\n",
              " 1010,\n",
              " 2029,\n",
              " 2511,\n",
              " 2014,\n",
              " 2004,\n",
              " 1037,\n",
              " 3948,\n",
              " 3063,\n",
              " 4969,\n",
              " 1010,\n",
              " 3687,\n",
              " 2274,\n",
              " 8922,\n",
              " 2982,\n",
              " 1998,\n",
              " 2956,\n",
              " 1996,\n",
              " 4908,\n",
              " 2980,\n",
              " 2531,\n",
              " 2193,\n",
              " 1011,\n",
              " 2028,\n",
              " 3895,\n",
              " 1000,\n",
              " 4689,\n",
              " 1999,\n",
              " 2293,\n",
              " 1000,\n",
              " 1998,\n",
              " 1000,\n",
              " 3336,\n",
              " 2879,\n",
              " 1000,\n",
              " 1012,\n",
              " 102,\n",
              " 2043,\n",
              " 2106,\n",
              " 20773,\n",
              " 2707,\n",
              " 3352,\n",
              " 2759,\n",
              " 1029,\n",
              " 102,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answer):\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answer)):\n",
        "    start_positions.append(encodings.char_to_token(i, answer[i]['answer_start']))\n",
        "    end_positions.append(encodings.char_to_token(i, answer[i]['answer_end']))\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length #Since there are a chances that context gets broken off to hit question in \n",
        "    go_back = -1\n",
        "    while end_positions[-1] is None:\n",
        "      end_positions[-1] = encodings.char_to_token(i, answer[i]['answer_end']-go_back)\n",
        "      go_back +=1\n",
        "  encodings.update({\n",
        "      'start_positions':start_positions,\n",
        "      'end_positions':end_positions\n",
        "  })\n",
        "add_token_positions(train_encoding, train_answers)\n",
        "add_token_positions(test_encoding, test_answers)\n",
        "train_encoding.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKa6ktePwajC",
        "outputId": "6d132af4-9d5d-4569-e0d3-f796cc9cb45b"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import tensorflow as tf\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings):\n",
        "    self.encodings = encodings \n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids) #This is how we prototype with torch. We are going to have to use our brains to figure out how to do\n",
        "    #this with tensorflow which is my goal."
      ],
      "metadata": {
        "id": "VnkKJTur20gm"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datasets = SquadDataset(train_encoding)\n",
        "val_datasets = SquadDataset(test_encoding)"
      ],
      "metadata": {
        "id": "paELv98z3A1S"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tuning"
      ],
      "metadata": {
        "id": "r3EJpSXM4bZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForQuestionAnswering\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQazSGLL5XPf",
        "outputId": "de1abdfa-9fe5-4976-c2e6-9bc2b5fe0b45"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm #shows progress of training"
      ],
      "metadata": {
        "id": "5RNPclUS4d3m"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "optim = AdamW(model.parameters(), lr = 5e-5)"
      ],
      "metadata": {
        "id": "qBerPWFE4xfS"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_datasets, batch_size = 16, shuffle=True)"
      ],
      "metadata": {
        "id": "szZFQuoQ5opp"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoding.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMa1jQc06sqz",
        "outputId": "8f8186f3-fc16-42a7-deed-8ce962c64125"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "  loop = tqdm(train_loader)\n",
        "  for batch in loop:\n",
        "    optim.zero_grad() #Resets gradients at start of each loop so after we update the models parameters accordingly we dont start from last point\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions = start_positions, end_positions = end_positions)\n",
        "\n",
        "    loss = outputs[0]\n",
        "    loss.backward() #gradient work\n",
        "    optim.step() \n",
        "\n",
        "    loop.set_description(f'Epoch{epoch}')\n",
        "    loop.set_postfix(loss = loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJP4yL3h6A7C",
        "outputId": "63187fd0-22f7-4170-d57e-7850af7e0f4b"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [01:04<00:00,  1.34s/it, loss=3.41]\n",
            "Epoch1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [01:04<00:00,  1.34s/it, loss=1.93]\n",
            "Epoch2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [01:04<00:00,  1.34s/it, loss=2.55]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "flqHWpjs7ict"
      },
      "execution_count": 125,
      "outputs": []
    }
  ]
}