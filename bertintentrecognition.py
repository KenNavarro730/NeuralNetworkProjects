# -*- coding: utf-8 -*-
"""BERTIntentRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1phOUqEtISFr2MtO4kVm8o0hZ9oOktn_I

WARNING: TENSORFLOW 2.0.0 COMPATIBLE, NEED TO DOWNGRADE TO RUN PROPERLY
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import math
import datetime 
from tqdm import tqdm 
import pandas as pd 
import numpy as np 
import tensorflow as tf 
import bert 
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights #We can find these in github source code and what they mean.
from bert.tokenization.bert_tokenization import FullTokenizer 
import seaborn as sns 
from pylab import rcParams 
import matplotlib.pyplot as plt 
from matplotlib.ticker import MaxNLocator
from matplotlib import rc
from sklearn.metrics import confusion_matrix, classification_report
# %matplotlib inline

np.random.seed(43)
tf.random.set_seed(43)

train = pd.read_csv("train.csv")
valid = pd.read_csv("valid.csv")
test = pd.read_csv("test.csv")



train['intent'].value_counts() #As we can see the dataset is pretty balanced which is sweet! Which means we dont have to do any data augmentation when it comes to sampling.

"""Here we are gonna append the validation data so when we use keras we can just do a .validation_split when we are fitting the neural network"""

train = train.append(valid).reset_index(drop=True)

train #resetting the index is essential! Otherwise we will just have indices repeating so iloc is automatically not a viable way to index things anymore so proper practice
#should be to reset index when appending.

"""Remember that in real life intent scenarios, we could use top k intents as some instances may fall into multiple categories relatively strong."""

os.makedirs("model", exist_ok=True)

!mv uncased_L-12_H-768_A-12/ model

bert_model_name = 'uncased_L-12_H-768_A-12'

bert_ckpt_dir = os.path.join("model/", bert_model_name)
bert_ckpt_file = os.path.join(bert_ckpt_dir, "bert_model.ckpt")
bert_config_file = os.path.join(bert_ckpt_dir, "bert_config.json")

class IntentDetectionData:
  data_column = 'text'
  target_column = 'intent'
  def __init__(self,train,test, tokenizer:FullTokenizer, classes, max_seq_len=192):
    self.tokenizer = tokenizer 
    self.max_seq_len=0
    self.classes = classes 
    train, test = map(lambda df: df.reindex(df[IntentDetectionData.data_column].str.len().sort_values().index),[train,test])
    ((self.trainx, self.trainy), (self.testx, self.testy)) =map(self._prepare, [train,test])
    self.max_seq_len = min(self.max_seq_len, max_seq_len)
    self.train_x,self.test_x = map(self._pad, [self.trainx, self.testx])
  def _prepare(self, df):
    x, y = [], []
    for _, row in tqdm(df.iterrows()):
      text, label = row[IntentDetectionData.data_column], row[IntentDetectionData.target_column]
      tokens = self.tokenizer.tokenize(text)
      tokens = ['[CLS]'] + tokens + ['[SEP]']
      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
      self.max_seq_len = max(self.max_seq_len, len(token_ids))
      x.append(token_ids)
      y.append(self.classes.index(label))
    return np.array(x), np.array(y)
  def _pad(self, ids):
    x = []
    for input_ids in ids:
      input_ids = input_ids[:min(len(input_ids), self.max_seq_len-2)] #line truncates size to max len size if too big -2 accounts for cls and sep
      input_ids = input_ids + [0]*(self.max_seq_len - len(input_ids))
      x.append(np.array(input_ids))
    return np.array(x)

tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, "vocab.txt"))

exampletokenizer = tokenizer.tokenize("Learning is my only enjoyable free time.")
vals = tokenizer.convert_tokens_to_ids(exampletokenizer)
vals

exampletokenizer

def create_model(max_seq_len, bert_config_file, bert_ckpt_file):

  with tf.io.gfile.GFile(bert_config_file, "r") as reader:
      bc = StockBertConfig.from_json_string(reader.read())
      bert_params = map_stock_config_to_params(bc)
      bert_params.adapter_size = None
      bert = BertModelLayer.from_params(bert_params, name="bert")
      #All this is doing is saying get the params from bert_config.json and instantiate the bert model layer with that.
      #Bert model layer is how we use a bert inside of another neural network with this api.
      #However not all bert apis are like this and some simply use different indices to indicate whether we want a neural network compatible1
      #or if we just want to use the pretrained naked without fine tuning.
  input_ids = tf.keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name = 'input_ids')
  bert_output = bert(input_ids)
  cls_output = tf.keras.layers.Lambda(lambda sep: sep[:, 0, :])(bert_output)
  cls_output = tf.keras.layers.Dropout(0.5)(cls_output)
  logits = tf.keras.layers.Dense(units=768, activation = 'tanh')(cls_output)
  logits = tf.keras.layers.Dropout(0.5)(logits)
  logits = tf.keras.layers.Dense(units=len(classes), activation = 'softmax')(logits)
  model = tf.keras.Model(inputs = input_ids, outputs = logits)
  model.build(input_shape=(max_seq_len, None))
        
  return model

classes = train.intent.unique().tolist()
data = IntentDetectionData(train, test, tokenizer, classes, max_seq_len=128)
model = create_model(data.max_seq_len, bert_config_file, bert_ckpt_file)
model.compile(optimizer = tf.keras.optimizers.Adam(1e-5),loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = tf.keras.metrics.SparseCategoricalAccuracy(name='acc'))
history = model.fit(
  x=data.train_x, 
  y=data.trainy,
  validation_split=0.1,
  batch_size=16,
  shuffle=True,
  epochs=5
)